{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Executive Summary"
      ],
      "metadata": {
        "id": "Z77PlKFcZ9SC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This report details the end-to-end development of a machine learning model to predict gender from footprint landmarks, fulfilling a request from a local police department. The project follows the CRISP-DM framework, encompassing data cleaning, extensive feature engineering, systematic model evaluation, and hyperparameter tuning.\n",
        "\n",
        "*   **Problem:** Predict gender with >90% accuracy from footprint data for forensic analysis.\n",
        "*   **Key Technique:** Advanced feature engineering based on podiatric research was the most critical factor for success.\n",
        "*   **Final Model:** An XGBoost classifier, tuned with Bayesian Optimization.\n",
        "*   **Result:** The final model achieved a **Kaggle private score of 0.9067**, successfully exceeding the 90% accuracy target and demonstrating a robust, explainable solution."
      ],
      "metadata": {
        "id": "lp9lNrywaKhM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt679ioAYxMc"
      },
      "source": [
        "## Business Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pNLC2lYw6m"
      },
      "source": [
        "The local police department has required the development of a binary prediction automated system that could determine the sex of individuals from the footprints that have been left at crime scenes, for the automated model, the local police force requires, needs to be able to make reasonably high predictions accuracy, within a limited of time, that will be used on a new device and to help the investigation team to narrow down suspects on the initial stages.\n",
        "\n",
        "To achieve these targets, we have been given a set of data that contains 18 landmarks in the form of X and y coordinates, the report below will provide a detailed examination of the data and its findings, the decision-making of each process, and recommendations for potential improvements and future work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmaHOTmnWSWt"
      },
      "source": [
        "## step 0: Prepareing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At step zero, we will first be setting up the necessary components for the work to work seamlessly and error free."
      ],
      "metadata": {
        "id": "YniBZlkNKISw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### local RUN setup"
      ],
      "metadata": {
        "id": "KxbJthZW7ZjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "VXqyorTS7ALe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle pandas joblib numpy matplotlib seaborn xgboost scipy statsmodels scikit-learn shap imblearn scikit-optimize"
      ],
      "metadata": {
        "id": "FvkQ3qKbIcbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle xgboost joblib statsmodels\n",
        "\n",
        "pip install --upgrade kaggle pandas joblib numpy matplotlib seaborn xgboost scipy statsmodels scikit-learn shap"
      ],
      "metadata": {
        "id": "bxRsVcKA6dtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade kaggle pandas joblib numpy matplotlib seaborn xgboost scipy statsmodels scikit-learn shap imblearn"
      ],
      "metadata": {
        "id": "ZqwG4Au-IfxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HYoYKaSZsxB"
      },
      "source": [
        "### Import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrREnhvZZr6T"
      },
      "outputs": [],
      "source": [
        "import kaggle\n",
        "import pandas as pd\n",
        "from joblib import dump, load\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "from scipy import stats\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, make_scorer, classification_report, confusion_matrix\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, normalize\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nua6BiAGPT7a"
      },
      "source": [
        "### funstion list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In function list it will hold all the implement function for features use and robustness."
      ],
      "metadata": {
        "id": "GWmgAZ_7OCzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_variants = {}\n",
        "X_test_variants = {}\n",
        "y_train_variants = {}\n",
        "y_test_variants = {}"
      ],
      "metadata": {
        "id": "TX7ePF7_e5eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_plot_importance(booster, figsize, **kwargs):\n",
        "    plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
        "    plot_importance(booster=booster)"
      ],
      "metadata": {
        "id": "xqz-Vrnel5YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_footprint(footprint_row, title):\n",
        "\n",
        "    x_values = [footprint_row[f'x{i}'] * width for i in range(18)]\n",
        "    y_values = [footprint_row[f'y{i}'] * height for i in range(18)]\n",
        "\n",
        "    plt.figure(figsize=(8, 12))\n",
        "    plt.scatter(x_values, y_values, color='red')\n",
        "\n",
        "    for i, (x, y) in enumerate(zip(x_values, y_values)):\n",
        "        plt.text(x, y, str(i), fontsize=12, color='black', ha='right', va='bottom')\n",
        "\n",
        "    plt.xlabel('Width (pixels)')\n",
        "    plt.ylabel('Height (pixels)')\n",
        "    plt.title(title)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iIDIg31j3Von"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "distance clataiton"
      ],
      "metadata": {
        "id": "6oXAMX9Ak3Rn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD0z5A64PW6T"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(df, x1, y1, x2, y2):\n",
        "    return np.sqrt((df[x1] - df[x2])**2 + (df[y1] - df[y2])**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lengths and widths"
      ],
      "metadata": {
        "id": "1uhfo1Ecl091"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lengths_widths_calculation(df):\n",
        "    df_lengths_widths = df.copy()\n",
        "    df_lengths_widths['lengths'] = euclidean_distance(df_lengths_widths, 'x1', 'y1', 'x9', 'y9')\n",
        "    df_lengths_widths['widths'] = euclidean_distance(df_lengths_widths, 'x5', 'y5', 'x15', 'y15')\n",
        "    return df_lengths_widths"
      ],
      "metadata": {
        "id": "7Mb3xPMrl0T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7 foot point"
      ],
      "metadata": {
        "id": "NQVXbOw7lBwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point7_calculation(df):\n",
        "    df_7_point_footprints = df.copy()\n",
        "    df_7_point_footprints['T1'] = euclidean_distance(df_7_point_footprints, 'x0', 'y0', 'x9', 'y9')\n",
        "    df_7_point_footprints['T2'] = euclidean_distance(df_7_point_footprints, 'x1', 'y1', 'x9', 'y9')\n",
        "    df_7_point_footprints['T3'] = euclidean_distance(df_7_point_footprints, 'x2', 'y2', 'x9', 'y9')\n",
        "    df_7_point_footprints['T4'] = euclidean_distance(df_7_point_footprints, 'x3', 'y3', 'x9', 'y9')\n",
        "    df_7_point_footprints['T5'] = euclidean_distance(df_7_point_footprints, 'x4', 'y4', 'x9', 'y9')\n",
        "    df_7_point_footprints['BAB'] = euclidean_distance(df_7_point_footprints, 'x5', 'y5', 'x15', 'y15')\n",
        "    df_7_point_footprints['BAH'] = euclidean_distance(df_7_point_footprints, 'x8', 'y8', 'x10', 'y10')\n",
        "\n",
        "    return df_7_point_footprints"
      ],
      "metadata": {
        "id": "buxKv9hXlBBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IQR missing value function"
      ],
      "metadata": {
        "id": "li7oVNWGlCC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IQR(df):\n",
        "    df_outlier_IQR = df.copy()\n",
        "    for column in df_outlier_IQR.columns:\n",
        "        Q1 = df_outlier_IQR[column].quantile(0.25)\n",
        "        Q3 = df_outlier_IQR[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 3 * IQR\n",
        "        upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "        df_outlier_IQR[column] = df_outlier_IQR[column].clip(lower_bound, upper_bound)\n",
        "\n",
        "    return df_outlier_IQR"
      ],
      "metadata": {
        "id": "QRa6Q-bhk-aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cap Outliers and Apply Robust and standard Scaling"
      ],
      "metadata": {
        "id": "BITlX0kmr-CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cap_outliers_and_scale(df):\n",
        "    df_outlier_capped_scale = df.copy()\n",
        "\n",
        "    for column in df_outlier_capped_scale:\n",
        "        Q1 = df_outlier_capped_scale[column].quantile(0.10)\n",
        "        Q3 = df_outlier_capped_scale[column].quantile(0.80)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        df_outlier_capped_scale[column] = np.clip(df_outlier_capped_scale[column], lower_bound, upper_bound)\n",
        "\n",
        "    robust_scaler = RobustScaler()\n",
        "    df_outlier_capped_scale = robust_scaler.fit_transform(df_outlier_capped_scale)\n",
        "\n",
        "    standard_scaler = StandardScaler()\n",
        "    df_outlier_capped_scale = standard_scaler.fit_transform(df_outlier_capped_scale)\n",
        "\n",
        "    return pd.DataFrame(df_outlier_capped_scale, columns=X.columns)"
      ],
      "metadata": {
        "id": "_4_rSujUr9SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Winsorization"
      ],
      "metadata": {
        "id": "VHTc1hYSdzht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Winsorization(df):\n",
        "    df_winsorized = df.copy()\n",
        "    for column in df_winsorized.columns:\n",
        "        df_winsorized[column] = winsorize(df_winsorized[column], limits=[0.003, 0.004])\n",
        "    return df_winsorized"
      ],
      "metadata": {
        "id": "fFsvmT4xcowH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "z_score"
      ],
      "metadata": {
        "id": "RjJbC5_3ieAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def z_score(df):\n",
        "    df_z_score = df.copy()\n",
        "    z_threshold = 4\n",
        "    for column in df_z_score.columns:\n",
        "        z_scores = stats.zscore(df_z_score[column])\n",
        "        df_z_score[column] = np.where(z_scores > z_threshold, df_z_score[column].median(), df_z_score[column])\n",
        "        df_z_score[column] = np.where(z_scores < -z_threshold, df_z_score[column].median(), df_z_score[column])\n",
        "    return df_z_score"
      ],
      "metadata": {
        "id": "ob4dOSGCiebd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "isolation_forest"
      ],
      "metadata": {
        "id": "wjtzA4IvFTeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def isolation_forest(df, contamination=0.05):\n",
        "    df_isolation = df.copy()\n",
        "    model = IsolationForest(contamination=contamination, random_state=42)\n",
        "\n",
        "    model.fit(df_isolation)\n",
        "\n",
        "    outlier_predictions = model.predict(df_isolation)\n",
        "\n",
        "    for column in df_isolation.columns:\n",
        "        median_value = df_isolation[column].median()\n",
        "        df_isolation[column] = np.where(outlier_predictions == -1, median_value, df_isolation[column])\n",
        "\n",
        "    return df_isolation"
      ],
      "metadata": {
        "id": "h82n69jDE8R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x30Tj_M0aPP5"
      },
      "source": [
        "## step 1: Understanding the data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tzw7XpakTLIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Although all landmarks are provided, it does not necessarily mean all of them will be positive for the model, therefore we will implement features engineering, This involves both adding new features and feature selection to improve model learning, details on feature engineering will be discussed in a later section.\n",
        "\n",
        "The data has been standardized between 0 and 1, if needed, we can recover to the original values by scaling back to 2240x3200, this will bring us back to its true data form, for more data understanding.\n",
        "\n",
        "The dataset contains 2,000 entries, which will be used to train the model and between them, x1 to y17 contain 6 to 17 missing values in between that require handling to ensure the data quality, and we will experiment with different imputation methods in step 3."
      ],
      "metadata": {
        "id": "OwldZqmCTMHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m3uSfjOZQMU"
      },
      "outputs": [],
      "source": [
        "footprints_data = pd.read_csv('SexLandmarks-train.csv')\n",
        "print(footprints_data.info())\n",
        "footprints_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWNz6n6JpEQe"
      },
      "outputs": [],
      "source": [
        "footprints_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, on \"Box Plots for Outliers\", outliers are present on the dataset, for early outlier handling, we can scale back the standardized data and calculate basic length and width, as it is difficult to gain meaningful information from the basic box plots, by doing so, we can identify extreme outliers more easily and correct them manually if needed, this method allows us to clean data more consistently, as leaving unreasonable extreme outliers most likely hurt the robustness of the dataset and effectiveness of the deployment."
      ],
      "metadata": {
        "id": "59RM_dTNjYq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "sns.boxplot(data=footprints_data)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Box Plots for Outliers\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7T2WoXU9TzHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "width, height = 2240, 3200\n",
        "\n",
        "original_scaled_data = footprints_data.copy()\n",
        "\n",
        "for column in original_scaled_data.columns:\n",
        "    if column.startswith('x'):\n",
        "        original_scaled_data[column] = original_scaled_data[column] * width\n",
        "    elif column.startswith('y'):\n",
        "        original_scaled_data[column] = original_scaled_data[column] * height\n",
        "\n",
        "print(original_scaled_data.head())"
      ],
      "metadata": {
        "id": "XUMYeAObXIiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "sns.boxplot(data=original_scaled_data)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Box Plots for Outliers\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "orT0_6fLXL0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_scaled_data_with_lengths_widths = lengths_widths_calculation(original_scaled_data)"
      ],
      "metadata": {
        "id": "-1u-4I9yqZ9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, group in original_scaled_data_with_lengths_widths.groupby('sex'):\n",
        "    plt.plot(group.lengths, group.widths, '.', label=name)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "zGgDC6NdXojk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph below shows the length and width of each footprint, Based on it we can observe extreme outliers, we will check if should we remove or correct these outliers, based on the landmark and dose it relistic."
      ],
      "metadata": {
        "id": "UiBkHTUVqqW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lengths_upper_threshold = original_scaled_data_with_lengths_widths['lengths'].quantile(0.95)\n",
        "lengths_lower_threshold = original_scaled_data_with_lengths_widths['lengths'].quantile(0.05)\n",
        "widths_upper_threshold = original_scaled_data_with_lengths_widths['widths'].quantile(0.95)\n",
        "widths_lower_threshold = original_scaled_data_with_lengths_widths['widths'].quantile(0.05)\n",
        "\n",
        "\n",
        "big_feet = original_scaled_data_with_lengths_widths[\n",
        "    (original_scaled_data_with_lengths_widths['lengths'] > lengths_upper_threshold) |\n",
        "    (original_scaled_data_with_lengths_widths['widths'] > widths_upper_threshold)\n",
        "]\n",
        "\n",
        "small_feet = original_scaled_data_with_lengths_widths[\n",
        "    (original_scaled_data_with_lengths_widths['lengths'] < lengths_lower_threshold) |\n",
        "    (original_scaled_data_with_lengths_widths['widths'] < widths_lower_threshold)\n",
        "]\n",
        "\n",
        "\n",
        "print(\"Big Feet Data Points:\")\n",
        "print(big_feet)\n",
        "\n",
        "print(\"\\nSmall Feet Data Points:\")\n",
        "print(small_feet)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "\n",
        "plt.scatter(original_scaled_data_with_lengths_widths['lengths'], original_scaled_data_with_lengths_widths['widths'], alpha=0.5, label='Normal Data')\n",
        "\n",
        "\n",
        "plt.scatter(big_feet['lengths'], big_feet['widths'], color='red', label='Big Feet Outliers', edgecolor='black')\n",
        "plt.scatter(small_feet['lengths'], small_feet['widths'], color='yellow', label='Small Feet Outliers', edgecolor='black')\n",
        "\n",
        "plt.xlabel('Lengths')\n",
        "plt.ylabel('Widths')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IQ0yodzCnMTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_foot_1 = small_feet[\n",
        "    (small_feet['lengths'] > 1600) & (small_feet['lengths'] < 1800) &\n",
        "    (small_feet['widths'] > 0) & (small_feet['widths'] < 500)\n",
        "]\n",
        "\n",
        "small_foot_2 = small_feet[\n",
        "    (small_feet['lengths'] > 2000) & (small_feet['lengths'] < 2150) &\n",
        "    (small_feet['widths'] > 400) & (small_feet['widths'] < 600)\n",
        "]\n",
        "\n",
        "big_foot_1 = big_feet[\n",
        "    (big_feet['lengths'] > 3100) & (big_feet['lengths'] < 3300) &\n",
        "    (big_feet['widths'] > 1900) & (big_feet['widths'] < 2100)\n",
        "]\n",
        "\n",
        "big_foot_2 = big_feet[\n",
        "    (big_feet['lengths'] > 2200) & (big_feet['lengths'] < 2300) &\n",
        "    (big_feet['widths'] > 100) & (big_feet['widths'] < 1100)\n",
        "]\n"
      ],
      "metadata": {
        "id": "ZJtqUBFCsJEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the graph below, the coordinates of the small feet, has shown a spread that are hardly can be recognized as human, therefore drop these data point from the dataset should improve the dataset.\n",
        "\n",
        "On the other hand both of the big foot seems to be showing a normal spared therefore they will be kept."
      ],
      "metadata": {
        "id": "osatvfV_4TLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_footprint(small_foot_1.iloc[0], 'Small Foot 1 (Length ~ 1780, Width ~ 200)')\n",
        "\n",
        "plot_footprint(small_foot_2.iloc[0], 'Small Foot 2 (Length ~ 2080, Width ~ 500)')\n",
        "\n",
        "plot_footprint(big_foot_1.iloc[0], 'Big Foot 1 (Length ~ 3200, Width ~ 2000)')\n",
        "\n",
        "plot_footprint(big_foot_2.iloc[0], 'Big Foot 2 (Length ~ 2230, Width ~ 1100)')"
      ],
      "metadata": {
        "id": "j3D52HeUvLKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_to_drop = [small_foot_1.index[0], small_foot_2.index[0]]\n",
        "\n",
        "footprints_data = footprints_data.drop(index=indices_to_drop)"
      ],
      "metadata": {
        "id": "yIqTIOvuwKy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "footprints_data.describe().T"
      ],
      "metadata": {
        "id": "rvVu3tQA4-a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph below shown there is a class imbalance on the dataset, it will be the best practice to implement the Synthetic Minority Over-sampling Technique (SMOTE) to prevent model bias. SMOTE will generate synthetic samples for the minority class, this can help to balance the dataset and improve the model's ability to generalize both classes."
      ],
      "metadata": {
        "id": "uFfYBQP16vX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHqNIrw4F1gP"
      },
      "outputs": [],
      "source": [
        "barplot=(sns.countplot(data= footprints_data, x='sex',hue='sex', palette=['b', 'g']))\n",
        "plt.title('0 v/s 1\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqwK7GbsIiNG"
      },
      "outputs": [],
      "source": [
        "corr = footprints_data.corr(method='spearman')\n",
        "\n",
        "triangle = np.triu(corr)\n",
        "\n",
        "plt.figure(figsize=(16, 7))\n",
        "sns.heatmap(data=corr, annot=True, mask=triangle, vmin=-1, vmax=1, cmap='RdBu_r', linewidths=.5, fmt= '.1f') # with 1 decimal precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ_YsLWsMALw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,12))\n",
        "sns.set_context('notebook',font_scale = 1)\n",
        "sns.heatmap(footprints_data.corr(),annot=True,linewidth =0.5)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEo0VnzROGI7"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(\n",
        "    corr,\n",
        "    vmin=-1, vmax=1, center=0,\n",
        "    cmap=sns.diverging_palette(20, 220, n=200),\n",
        "    square=True\n",
        ")\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation=45,\n",
        "    horizontalalignment='right'\n",
        ");\n",
        "ax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Th is dataset has shown there is no duplicated, therefore no action needed"
      ],
      "metadata": {
        "id": "XyTk6iwf8-AS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53IRWg1BHtWL"
      },
      "outputs": [],
      "source": [
        "footprints_data.duplicated().value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdUpWTfQ_KD0"
      },
      "source": [
        "## step 2: data processing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TrH4JtZbW5q"
      },
      "source": [
        "### outliers handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUgYM8y2Ydq_"
      },
      "outputs": [],
      "source": [
        "footprints_data_df = footprints_data.copy()\n",
        "footprints_data_df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryj99SLagWIJ"
      },
      "source": [
        "we will uses 4 method to handle outliers, and we will not be dropping outliers, because as seen there is meaningful data with in the outliers, therefore Dropping them could bring loss of important patterns.\n",
        "\n",
        "1.Basic IQR Method:\n",
        "* The Interquartile Range (IQR) is a standard technique used to identify outliers, the outliers will be capped to a bounds, to limit their range.\n",
        "\n",
        "2.Cap Outliers and Apply Robust Scaling\n",
        "* Similar to the IQR method but apply robusts and standard scaling to create deviation of the data.\n",
        "\n",
        "3.Winsorization\n",
        "* limits exteme values by capping them within specified boundaries.\n",
        "\n",
        "4.Use Z score\n",
        "* uses standard deviation to identify outliers, which are then replaced replaced with the median to reduce their effect."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "footprints_data_df = footprints_data.copy()"
      ],
      "metadata": {
        "id": "uz5dISXhcu7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGU_g_XVAksx"
      },
      "source": [
        " #### use IQR for outliners"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "footprints_data_df = footprints_data.copy()\n",
        "footprints_data_df.describe().T"
      ],
      "metadata": {
        "id": "dzOXwU8PvIm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = footprints_data_df.drop('sex', axis=1)\n",
        "y = footprints_data_df['sex']"
      ],
      "metadata": {
        "id": "jBJJNWg_vQhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNNmLYpheInP"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train_iqr = IQR(X_train)\n",
        "X_test_iqr = IQR(X_test)\n",
        "\n",
        "y_train_iqr = y_train.loc[X_train_iqr.index]\n",
        "y_test_iqr = y_test.loc[X_test_iqr.index]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_variants['IQR'] = X_train_iqr\n",
        "X_test_variants['IQR'] = X_test_iqr\n",
        "y_train_variants['IQR'] = y_train_iqr\n",
        "y_test_variants['IQR'] = y_test_iqr"
      ],
      "metadata": {
        "id": "YQaTx3hhe2w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUrG0neubIIU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "lengths_widths_df_iqr = pd.concat([X_train_iqr, y_train_iqr], axis=1)\n",
        "\n",
        "for name, group in lengths_widths_df_iqr.groupby('sex'):\n",
        "    plt.plot(group.x1, group.x14, '.', label=name)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_iqr.describe()"
      ],
      "metadata": {
        "id": "XNtduyw77Xvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW-w0JUGA221"
      },
      "source": [
        "#### Cap Outliers and Apply Robust and standard Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PanyprDh0SJ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train_robust = cap_outliers_and_scale(X_train)\n",
        "X_test_robust = cap_outliers_and_scale(X_test)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "y_train_robust = y_train.loc[X_train_robust.index]\n",
        "y_test_robust = y_test.loc[X_test_robust.index]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_variants['RobustScaling'] = X_train_robust\n",
        "X_test_variants['RobustScaling'] = X_test_robust\n",
        "y_train_variants['RobustScaling'] = y_train_robust\n",
        "y_test_variants['RobustScaling'] = y_test_robust"
      ],
      "metadata": {
        "id": "K-oOdI7yfNnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths_widths_df_robust = pd.concat([X_train_robust, y_train_robust], axis=1)\n",
        "\n",
        "for name, group in lengths_widths_df_robust.groupby('sex'):\n",
        "    plt.plot(group.x1, group.x14, '.', label=name)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "RBHhLNUgZHAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmMomRdsv93W"
      },
      "outputs": [],
      "source": [
        "lengths_widths_df_robust.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSMi0iVajNZn"
      },
      "source": [
        "for now we have done with the grouping onto lengths\tand widths which we have mentioned earlier and we have use 2 ways to deal with outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Winsorization\n"
      ],
      "metadata": {
        "id": "2rbeIYigDJiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train_Winsorization = Winsorization(X_train)\n",
        "X_test_Winsorization = Winsorization(X_test)\n",
        "\n",
        "y_train_Winsorization = y_train.loc[X_train_Winsorization.index]\n",
        "y_test_Winsorization = y_test.loc[X_test_Winsorization.index]"
      ],
      "metadata": {
        "id": "OZrFAORXEK3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_variants['Winsorization'] = X_train_Winsorization\n",
        "X_test_variants['Winsorization'] = X_test_Winsorization\n",
        "y_train_variants['Winsorization'] = y_train_Winsorization\n",
        "y_test_variants['Winsorization'] = y_test_Winsorization"
      ],
      "metadata": {
        "id": "053lNemfhBud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths_widths_df_Winsorization = pd.concat([X_train_Winsorization, y_train_Winsorization], axis=1)\n",
        "\n",
        "for name, group in lengths_widths_df_Winsorization.groupby('sex'):\n",
        "    plt.plot(group.x1, group.x14, '.', label=name)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "lrD80vlv9TbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths_widths_df_Winsorization.describe()"
      ],
      "metadata": {
        "id": "aP_clMLogocP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### z_score_df\n"
      ],
      "metadata": {
        "id": "cZ94LQW6EKg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train_z_score = z_score(X_train)\n",
        "X_test_z_score = z_score(X_test)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "y_train_z_score = y_train.loc[X_train_robust.index]\n",
        "y_test_z_score = y_test.loc[X_test_robust.index]"
      ],
      "metadata": {
        "id": "E1EVyDgHC6Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_variants['z_score'] = X_train_z_score\n",
        "X_test_variants['z_score'] = X_test_z_score\n",
        "y_train_variants['z_score'] = y_train_z_score\n",
        "y_test_variants['z_score'] = y_test_z_score"
      ],
      "metadata": {
        "id": "_Ucxj4kJimIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths_widths_df_z_score = pd.concat([X_train_z_score, y_train_z_score], axis=1)\n",
        "\n",
        "for name, group in lengths_widths_df_z_score.groupby('sex'):\n",
        "    plt.plot(group.x1, group.x14, '.', label=name)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "DgARijIwE2h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths_widths_df_z_score.describe()"
      ],
      "metadata": {
        "id": "niVuGtzigx0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## step 3: missing value handle"
      ],
      "metadata": {
        "id": "Mt2dePne1b3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this step, we group the data that has been processed for outliers handling, assign key values for easier management, then we apply KNN Imputer and Iterative Imputer, this avoids data leakage meanwhile being efficient.\n",
        "These two imputation methods are chosen because:\n",
        "\n",
        "1. KNN Imputer:\n",
        "* The KNN fills up missing values by averaging the values from the nearest    \n",
        "neighbours, this helps missing values while keeping the patterns related to those neighbours.\n",
        "\n",
        "2. Iterative Imputer:\n",
        "* The Iterative predicts each missing value by running an iterative regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "cNxdPIShDKWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {\n",
        "    'IQR': (X_train_iqr, X_test_iqr, y_train_iqr, y_test_iqr),\n",
        "    'RobustScaling': (X_train_robust, X_test_robust, y_train_robust, y_test_robust),\n",
        "    'Winsorization': (X_train_Winsorization, X_test_Winsorization, y_train_Winsorization, y_test_Winsorization),\n",
        "    'Zscore': (X_train_z_score, X_test_z_score, y_train_z_score, y_test_z_score),\n",
        "}\n",
        "\n",
        "KNNImputer = KNNImputer(n_neighbors=4)\n",
        "IterativeImputer = IterativeImputer(max_iter=6, random_state=0)\n",
        "\n",
        "imputed_variants = {}\n",
        "\n",
        "for variant_name, (X_train, X_test, y_train, y_test) in datasets.items():\n",
        "\n",
        "    X_train_imputed = pd.DataFrame(IterativeImputer.fit_transform(X_train), columns=X_train.columns)\n",
        "    X_test_imputed = pd.DataFrame(IterativeImputer.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "    key = f\"{variant_name}_Iterative\"\n",
        "    imputed_variants[key] = (X_train_imputed, X_test_imputed, y_train, y_test)\n",
        "\n",
        "for variant_name, (X_train, X_test, y_train, y_test) in datasets.items():\n",
        "\n",
        "    X_train_imputed = pd.DataFrame(KNNImputer.fit_transform(X_train), columns=X_train.columns)\n",
        "    X_test_imputed = pd.DataFrame(KNNImputer.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "    key = f\"{variant_name}_knn\"\n",
        "    imputed_variants[key] = (X_train_imputed, X_test_imputed, y_train, y_test)\n",
        "\n",
        "print(imputed_variants.keys())"
      ],
      "metadata": {
        "id": "O6RzM14y9ZN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KePhREKU22Vt"
      },
      "source": [
        "## step 4: baseline test before features engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we will prepare a baseline test for the performance of the models, as at this point we have already cleaned up our data with the basic method we have covered, and now the data are already for a baseline test and we will choose models that perform well for further development."
      ],
      "metadata": {
        "id": "XpMokQxQggyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
        "    \"Gaussian Naïve Bayes\": GaussianNB(),\n",
        "    \"Support Vector Machine\": SVC(kernel='rbf', degree=3, gamma='scale', max_iter=1000),\n",
        "    \"KNN Classifier\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"SGDOneClassSVM\":linear_model.SGDOneClassSVM()\n",
        "}"
      ],
      "metadata": {
        "id": "nw-kuxxtpeo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results from the baseline test has shown a acceptable performance concider we have only processed with basic methodology to clean the data.\n",
        "\n",
        "The list below has shown the results in order of accuracy score, along with the model, variant of the dataset, outliers method and imputation method. According to the results, the best-performing model so far is XGBoost, which uses IQR_Iterative and it able to achieve 0.8250, This suggests further development of XGBoost will be worthwhile, followed by Random Forest and Gradient Boosting, with a different set of variants, it has also shown there is not yet have a clear idea of which variants will be the best for us to achieve our goal there for more test will be needed in future steps."
      ],
      "metadata": {
        "id": "by_cSC9Zim_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    for dataset_name, (X_train, X_test, y_train, y_test) in imputed_variants.items():\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Variant\": dataset_name,\n",
        "            \"Outlier Handling Method\": dataset_name.split('_')[0],\n",
        "            \"Imputation Method\": dataset_name.split('_')[1],\n",
        "            \"Accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "results_df = results_df.sort_values(by=\"Accuracy\", ascending=False)\n",
        "\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "oDf8-jawHFsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## step 5: features engineering"
      ],
      "metadata": {
        "id": "CGyt_7Q3mgJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this step we will implement two different kinds of feature engineering, first, we will simply calculate the fundamental lengths and widths of the footprints, to obtain extra measurements that can help understanding of the size.\n",
        "\n",
        "Second approach involves using a more unique feature extraction technique based on research of Abledu et al. (2015), published by the NIH (National Library of Medicine), they have implemented an calculation of Seven dimensions–length of each toe to the bottom (t1 to t5), breadth at the ball (BAB) and breadth at heel (BAH), will this approach they have able to achieve a remarkable accuracy in a similar tasks, therefore we will implement this along with the basic lengths and widths calculation.\n",
        "\n",
        "ref\n",
        "\n",
        "Abledu, J. K., Abledu, G. K., Offei, E. B., and Antwi, E. M., 2015. Determination of sex from footprint dimensions in a Ghanaian population [online]. PloS one. Available from: https://pmc.ncbi.nlm.nih.gov/articles/PMC4596846/ [Accessed 5 Nov 2024]."
      ],
      "metadata": {
        "id": "e1qdnMADq00-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_engineered_variants = {}\n",
        "\n",
        "for variant_name, (X_train, X_test, y_train, y_test) in imputed_variants.items():\n",
        "\n",
        "    X_train_lengths = lengths_widths_calculation(X_train)\n",
        "    X_test_lengths = lengths_widths_calculation(X_test)\n",
        "\n",
        "    key = f\"{variant_name}_lengths_widths\"\n",
        "    feature_engineered_variants[key] = (X_train_lengths, X_test_lengths, y_train, y_test)\n",
        "\n",
        "for variant_name, (X_train, X_test, y_train, y_test) in imputed_variants.items():\n",
        "\n",
        "    X_train_point7 = point7_calculation(X_train)\n",
        "    X_test_point7 = point7_calculation(X_test)\n",
        "\n",
        "    key = f\"{variant_name}_point7\"\n",
        "    feature_engineered_variants[key] = (X_train_point7, X_test_point7, y_train, y_test)\n",
        "\n",
        "print(feature_engineered_variants.keys())"
      ],
      "metadata": {
        "id": "eqdGqNHvmm_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km7xSTT30u_n"
      },
      "source": [
        "## step 6: Testing all the model after features engineering (baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have now implement features engineering, it will be beneficial to did an other baseline test to have a better understanding dose the features we create  bring positive or negative impact to the model learning"
      ],
      "metadata": {
        "id": "H9Lo5CJ7PSX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_point7, X_test_point7, y_train_point7, y_test_point7 = feature_engineered_variants['Winsorization_Iterative_lengths_widths']"
      ],
      "metadata": {
        "id": "0TqU5LTLPmOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI1anCFPXmsN"
      },
      "outputs": [],
      "source": [
        "engineered_results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    for dataset_name, (X_train, X_test, y_train, y_test) in feature_engineered_variants.items():\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        engineered_results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Variant\": dataset_name,\n",
        "            \"Outlier Handling Method\": dataset_name.split('_')[0],\n",
        "            \"Imputation Method\": dataset_name.split('_')[1],\n",
        "            \"Feature Engineering\": dataset_name.split('_')[2],\n",
        "            \"Accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "engineered_results_df = pd.DataFrame(engineered_results)\n",
        "\n",
        "engineered_results_df = engineered_results_df.sort_values(by=\"Accuracy\", ascending=False)\n",
        "\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "print(engineered_results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e8RTARCYNMz"
      },
      "source": [
        "## step 7: Hyperparameter Tuning For model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Imputed Variants:\")\n",
        "print(imputed_variants.keys())\n",
        "\n",
        "print(\"Feature-Engineered Variants:\")\n",
        "print(feature_engineered_variants.keys())\n",
        "\n",
        "print(\"Holdout Imputed Variants:\")\n",
        "print(imputed_variants_holdout.keys())\n",
        "\n",
        "print(\"Holdout Feature-Engineered Variants:\")\n",
        "print(holdout_feature_engineered_variants.keys())"
      ],
      "metadata": {
        "id": "VtHm2apDUIv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgNG4PsJF4NN"
      },
      "source": [
        "#### Hyperparameter Tuning for XGBoost (kaggle 0.8334)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "best_variant_name = 'Winsorization_knn_point7'\n",
        "X_train_best, X_test_best, y_train_best, y_test_best = feature_engineered_variants[best_variant_name]\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7, 9, 11],\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'subsample': [0.5, 0.7, 0.9, 1],\n",
        "    'colsample_bytree': [0.5, 0.7, 0.9, 1],\n",
        "    'colsample_bylevel': [0.5, 0.7, 0.9, 1],\n",
        "    'colsample_bynode': [0.5, 0.7, 0.9, 1],\n",
        "    'min_child_weight': [1, 3, 5, 7, 10],\n",
        "    'gamma': [0, 0.1, 0.3, 0.5, 1],\n",
        "    'reg_lambda': [0.5, 1, 1.5],\n",
        "    'reg_alpha': [0, 0.5, 1, 1.5],\n",
        "    'booster': ['gbtree', 'dart'],\n",
        "    'tree_method': ['auto', 'exact', 'hist']\n",
        "}\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "grid_search = RandomizedSearchCV(\n",
        "    xgb, param_grid, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, verbose=2, random_state=42\n",
        ")\n",
        "grid_search.fit(X_train_best, y_train_best)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy from Grid Search:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "YxBwF_U0mAFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35khQsazFq5r"
      },
      "outputs": [],
      "source": [
        "best_variant_name = 'Winsorization_Iterative_lengths_widths'\n",
        "X_train_best, X_test_best, y_train_best, y_test_best = feature_engineered_variants[best_variant_name]\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.05, 0.1, 0.3],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.5, 0.6, 1.0],\n",
        "    'colsample_bytree': [0.5, 0.7, 0.9, 1.0],\n",
        "    'gamma': [0, 0.1, 0.3, 0.5, 1],\n",
        "    'min_child_weight': [1, 3, 5, 7],\n",
        "    'reg_alpha': [0, 0.1, 1],\n",
        "    'reg_lambda': [0.5, 1, 2, 5]\n",
        "}\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "grid_search = GridSearchCV(xgb, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_best, y_train_best)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy from Grid Search:\", grid_search.best_score_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test_best)\n",
        "accuracy_best = accuracy_score(y_test_best, y_pred_best)\n",
        "print(f\"Test Accuracy for Best Model: {accuracy_best:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomizedSearchCV first than GridSearchCV to safe time as there will be less to try on and close down the candidates"
      ],
      "metadata": {
        "id": "pdrB78GG7R1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbeWSz6ETR5g"
      },
      "outputs": [],
      "source": [
        "best_variant_name = 'Winsorization_knn_point7'\n",
        "X_train, X_test, y_train, y_test = feature_engineered_variants[best_variant_name]\n",
        "print(f\"Running model for variant: {best_variant_name}\")\n",
        "\n",
        "all_accuracies = []\n",
        "num_runs = 10\n",
        "\n",
        "for i in range(num_runs):\n",
        "    X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=i\n",
        "    )\n",
        "\n",
        "    print(f\"Features before fitting (run {i + 1}): {X_train_best.columns}\")\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        n_estimators=100,\n",
        "        subsample=0.5,\n",
        "        eval_metric='logloss',\n",
        "        reg_lambda=0.5,\n",
        "        reg_alpha=1,\n",
        "        min_child_weight=5,\n",
        "        gamma=0.1,\n",
        "        colsample_bytree=0.9,\n",
        "        random_state=i,\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_best, y_train_best)\n",
        "    y_pred = model.predict(X_test_best)\n",
        "\n",
        "    accuracy = accuracy_score(y_test_best, y_pred)\n",
        "    all_accuracies.append(accuracy)\n",
        "\n",
        "    scores = cross_validate(\n",
        "        model, X_train_best, y_train_best, cv=5, return_train_score=True, return_estimator=True\n",
        "    )\n",
        "\n",
        "    print(f\"Run {i + 1}:\")\n",
        "    print(f\"Accuracy (Testing): {accuracy:.2f}\")\n",
        "    print(f\"Accuracy (CV Mean): {np.mean(scores['test_score']):.2f} (+/- {np.std(scores['test_score']) * 2:.2f})\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_best, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSummary of accuracies across runs:\")\n",
        "print(f\"Mean accuracy over {num_runs} runs: {np.mean(all_accuracies):.2f} (+/- {np.std(all_accuracies):.2f})\")\n",
        "\n",
        "print(classification_report(y_test_best, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "training to test on the robustness of the process we are getting 85% with almost the same CV mean which means it is generalising well"
      ],
      "metadata": {
        "id": "rDnfjTCw78zZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8XLmRJqXFR5"
      },
      "outputs": [],
      "source": [
        "best_xgb = XGBClassifier(\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    n_estimators=100,\n",
        "    subsample=0.5,\n",
        "    eval_metric='logloss',\n",
        "    reg_lambda=0.5,\n",
        "    reg_alpha=1,\n",
        "    min_child_weight=5,\n",
        "    gamma=0.1,\n",
        "    colsample_bytree=0.9,\n",
        ")\n",
        "\n",
        "best_xgb.fit(X_train_best, y_train_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model output for more data understanding later"
      ],
      "metadata": {
        "id": "Ci-b_m807_bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### XGBoost play ground"
      ],
      "metadata": {
        "id": "GC102_UOfaY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_plot_importance(booster, figsize, **kwargs):\n",
        "    plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
        "    plot_importance(booster=booster)\n",
        "\n",
        "my_plot_importance(best_xgb, figsize=(10,10), importance_type='gain')"
      ],
      "metadata": {
        "id": "Uc3-Xgsnc0Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(best_xgb)\n",
        "shap_values = explainer.shap_values(X_test_best)\n",
        "shap.summary_plot(shap_values, X_test_best)"
      ],
      "metadata": {
        "id": "UG9S4HnApy_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "result = permutation_importance(best_xgb, X_test_best, y_test_best, n_repeats=10, random_state=42)\n",
        "for i in result.importances_mean.argsort()[::-1]:\n",
        "    print(f\"{X_test_best.columns[i]}: {result.importances_mean[i]:.4f} +/- {result.importances_std[i]:.4f}\")"
      ],
      "metadata": {
        "id": "Zp0jSU-9qDVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "up to now we can see which features are more imporatnat which are not, it will allow us to do features selection, base on the infrmation above"
      ],
      "metadata": {
        "id": "1eOXQEjh8Urw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_imbalance = pd.concat([X_train_best, y_train_best], axis=1)\n",
        "\n",
        "for name, group in data_imbalance.groupby('sex'):\n",
        "    plt.plot(group.BAB, group.HB_index, '.', label=name)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "QSe9eB_i8u6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot=(sns.countplot(data= data_imbalance, x='sex',hue='sex', palette=['b', 'g']))\n",
        "plt.title('0 v/s 1\\n')"
      ],
      "metadata": {
        "id": "ufYclgHktpmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as shown above there is heavy data imblance and there is ouliers with in the engineered features, to move forword for better XGBoost performacne we will implnemnt 3 different ways for outliners and ways uses SMOTE for class imblance than perform feature selection and compaire there perfomance together"
      ],
      "metadata": {
        "id": "hU3s4kTpR54O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = feature_engineered_variants['Winsorization_knn_point7']\n",
        "\n",
        "X_train_point7_iqr = IQR(X_train)\n",
        "X_test_point7_iqr = IQR(X_test)\n",
        "\n",
        "y_train_point7_iqr = y_train.loc[X_train_iqr.index]\n",
        "y_test_point7_iqr = y_test.loc[X_test_iqr.index]\n"
      ],
      "metadata": {
        "id": "a5wgzhkt_sPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point7_df_iqr = pd.concat([X_train_point7_iqr, y_train_point7_iqr], axis=1)\n",
        "\n",
        "for name, group in point7_df_iqr.groupby('sex'):\n",
        "    plt.plot(group.BAB, group.HB_index, '.', label=name)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "4vzHOD0LCspw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_point7_z_score = z_score(X_train)\n",
        "X_test_point7_z_score = z_score(X_test)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "y_train_point7_z_score = y_train.loc[X_train_robust.index]\n",
        "y_test_point7_z_score = y_test.loc[X_test_robust.index]"
      ],
      "metadata": {
        "id": "p5aYwlwjCs8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point7_df_z_score = pd.concat([X_train_point7_z_score, y_train_point7_z_score], axis=1)\n",
        "\n",
        "for name, group in point7_df_z_score.groupby('sex'):\n",
        "    plt.plot(group.BAB, group.HB_index, '.', label=name)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "DQNPuSNiDiQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_point7_isolation_forest = isolation_forest(X_train)\n",
        "X_test_point7_isolation_forest = isolation_forest(X_test)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "y_train_point7_isolation_forest = y_train.loc[X_train_point7_isolation_forest.index]\n",
        "y_test_point7_isolation_forest = y_test.loc[X_test_point7_isolation_forest.index]"
      ],
      "metadata": {
        "id": "7oYClM97Dn6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "point7_df_isolation_forest = pd.concat([X_train_point7_isolation_forest, y_train_point7_isolation_forest], axis=1)\n",
        "\n",
        "for name, group in point7_df_isolation_forest.groupby('sex'):\n",
        "    plt.plot(group.BAB, group.HB_index, '.', label=name)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "6Aml5jWrHBpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "do the same for hold out"
      ],
      "metadata": {
        "id": "og7dg2_Q7hba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we have done all 3 ways that we have talked about for daeling with the ouliners, and each of them have perfrom abit different which it will be provide a good variants on the outcome"
      ],
      "metadata": {
        "id": "SiqzPWstHPvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, RandomOverSampler\n",
        "from imblearn.combine import SMOTETomek"
      ],
      "metadata": {
        "id": "9V2XTzCFnB6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, RandomOverSampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "point7_datasets = {\n",
        "    'point7_IQR': (X_train_point7_iqr, X_test_point7_iqr, y_train_point7_iqr, y_test_point7_iqr),\n",
        "    'point7_Zscore': (X_train_point7_z_score, X_test_point7_z_score, y_train_point7_z_score, y_test_point7_z_score),\n",
        "    'point7_isolationforest': (X_train_point7_isolation_forest, X_test_point7_isolation_forest, y_train_point7_isolation_forest, y_test_point7_isolation_forest)\n",
        "}\n",
        "\n",
        "XGBoost_outliers_variants = {}\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in point7_datasets.items():\n",
        "\n",
        "    smote = SMOTE()\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    X_train_imputed = X_train_resampled\n",
        "    X_test_imputed = X_test\n",
        "\n",
        "    key = f\"{dataset_name}_SMOTE\"\n",
        "    XGBoost_outliers_variants[key] = (X_train_imputed, X_test_imputed, y_train_resampled, y_test)\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in point7_datasets.items():\n",
        "\n",
        "    smote = BorderlineSMOTE()\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    X_train_imputed = X_train_resampled\n",
        "    X_test_imputed = X_test\n",
        "\n",
        "    key = f\"{dataset_name}_BorderlineSMOTE\"\n",
        "    XGBoost_outliers_variants[key] = (X_train_imputed, X_test_imputed, y_train_resampled, y_test)\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in point7_datasets.items():\n",
        "\n",
        "    smote = SVMSMOTE()\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    X_train_imputed = X_train_resampled\n",
        "    X_test_imputed = X_test\n",
        "\n",
        "    key = f\"{dataset_name}_SVMSMOTE\"\n",
        "    XGBoost_outliers_variants[key] = (X_train_imputed, X_test_imputed, y_train_resampled, y_test)\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in point7_datasets.items():\n",
        "\n",
        "    smote = RandomOverSampler(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    X_train_imputed = X_train_resampled\n",
        "    X_test_imputed = X_test\n",
        "\n",
        "    key = f\"{dataset_name}_RandomSMOTE\"\n",
        "    XGBoost_outliers_variants[key] = (X_train_imputed, X_test_imputed, y_train_resampled, y_test)\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in point7_datasets.items():\n",
        "\n",
        "    smote = SMOTETomek(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    X_train_imputed = X_train_resampled\n",
        "    X_test_imputed = X_test\n",
        "\n",
        "    key = f\"{dataset_name}_SMOTETomek\"\n",
        "    XGBoost_outliers_variants[key] = (X_train_imputed, X_test_imputed, y_train_resampled, y_test)\n",
        "\n",
        "print(XGBoost_outliers_variants.keys())"
      ],
      "metadata": {
        "id": "8XbLhfxu_sxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_point7_IQR_SMOTE, X_test_point7_IQR_SMOTE, y_train_point7_IQR_SMOTE, y_test_point7_IQR_SMOTE = XGBoost_outliers_variants['point7_IQR_SMOTE']\n",
        "\n",
        "train_data_point7_IQR_SMOTE = pd.concat([X_train_point7_IQR_SMOTE, y_train_point7_IQR_SMOTE], axis=1)\n",
        "test_data_point7_IQR_SMOTE = pd.concat([X_test_point7_IQR_SMOTE, y_test_point7_IQR_SMOTE], axis=1)\n",
        "\n",
        "barplot=(sns.countplot(data= train_data_point7_IQR_SMOTE, x='sex',hue='sex', palette=['b', 'g']))\n",
        "barplot=(sns.countplot(data= test_data_point7_IQR_SMOTE, x='sex',hue='sex', palette=['b', 'g']))\n",
        "plt.title('0 v/s 1\\n')"
      ],
      "metadata": {
        "id": "uzeRu2AVt9Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as shown the smote is applied only to the test set to avoid data leakage  "
      ],
      "metadata": {
        "id": "cgX4WCh2PhzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "point7_smote_engineered_results_df = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    for dataset_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_variants.items():\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        point7_smote_engineered_results_df.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Variant\": dataset_name,\n",
        "            \"Feature Engineering\": dataset_name.split('_')[0],\n",
        "            \"Imputation Method\": dataset_name.split('_')[1],\n",
        "            \"Smote Method\": dataset_name.split('_')[2],\n",
        "            \"Accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "point7_smote_engineered_df = pd.DataFrame(point7_smote_engineered_results_df)\n",
        "\n",
        "point7_smote_engineered_results_df = point7_smote_engineered_df.sort_values(by=\"Accuracy\", ascending=False)\n",
        "\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(point7_smote_engineered_results_df)"
      ],
      "metadata": {
        "id": "ZHhpNCO9SdI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for now the Accuracy seems like the same as before but we should try on XGBoost to get more inforemation about its performacne"
      ],
      "metadata": {
        "id": "rEzQWGzdlx7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "best_variant_name = 'point7_Zscore_SMOTE'\n",
        "X_train, X_test, y_train, y_test = XGBoost_outliers_variants[best_variant_name]\n",
        "print(f\"Running model for variant: {best_variant_name}\")\n",
        "\n",
        "all_accuracies = []\n",
        "num_runs = 10\n",
        "\n",
        "for i in range(num_runs):\n",
        "    X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=i\n",
        "    )\n",
        "\n",
        "    print(f\"Features before fitting (run {i + 1}): {X_train_best.columns}\")\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        n_estimators=100,\n",
        "        subsample=0.5,\n",
        "        eval_metric='logloss',\n",
        "        reg_lambda=0.5,\n",
        "        reg_alpha=1,\n",
        "        min_child_weight=5,\n",
        "        gamma=0.1,\n",
        "        colsample_bytree=0.9,'\n",
        "        random_state=43,\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_best, y_train_best)\n",
        "    y_pred = model.predict(X_test_best)\n",
        "\n",
        "    accuracy = accuracy_score(y_test_best, y_pred)\n",
        "    all_accuracies.append(accuracy)\n",
        "\n",
        "    scores = cross_validate(\n",
        "        model, X_train_best, y_train_best, cv=5, return_train_score=True, return_estimator=True\n",
        "    )\n",
        "\n",
        "    print(f\"Run {i + 1}:\")\n",
        "    print(f\"Accuracy (Testing): {accuracy:.2f}\")\n",
        "    print(f\"Accuracy (CV Mean): {np.mean(scores['test_score']):.2f} (+/- {np.std(scores['test_score']) * 2:.2f})\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_best, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSummary of accuracies across runs:\")\n",
        "print(f\"Mean accuracy over {num_runs} runs: {np.mean(all_accuracies):.2f} (+/- {np.std(all_accuracies):.2f})\")\n",
        "print(classification_report(y_test_best, y_pred))"
      ],
      "metadata": {
        "id": "9j6RoXkWZu2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see our performance have largely increased on different areas with out features selection, now we will move onto features selection."
      ],
      "metadata": {
        "id": "zGXDylwNN50L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote_best_xgb = XGBClassifier(\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    n_estimators=100,\n",
        "    subsample=0.5,\n",
        "    eval_metric='logloss',\n",
        "    reg_lambda=0.5,\n",
        "    reg_alpha=1,\n",
        "    min_child_weight=5,\n",
        "    gamma=0.1,\n",
        "    colsample_bytree=0.9,\n",
        ")\n",
        "\n",
        "smote_best_xgb.fit(X_train_best, y_train_best)"
      ],
      "metadata": {
        "id": "oWXfjHLalsrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(smote_best_xgb)\n",
        "shap_values = explainer.shap_values(X_test_best)\n",
        "shap.summary_plot(shap_values, X_test_best)"
      ],
      "metadata": {
        "id": "8KNsbfxymXzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that even after we done isoforest and smotie, it stay the same as before because (look at i can talk about for smote)"
      ],
      "metadata": {
        "id": "ywbDvTh_pWbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_plot_importance(booster, figsize, **kwargs):\n",
        "    plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
        "    plot_importance(booster=booster)\n",
        "\n",
        "my_plot_importance(smote_best_xgb, figsize=(10,10), importance_type='gain')"
      ],
      "metadata": {
        "id": "dSNE7cyyoB90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectFromModel, RFE, RFECV, SequentialFeatureSelector\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "XGBoost_outliers_variants_features_selected = {}\n",
        "\n",
        "threshold_importance = 0.9\n",
        "n_features_to_select = 25\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_variants.items():\n",
        "    model = XGBClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    importance_scores = model.get_booster().get_score(importance_type='gain')\n",
        "\n",
        "    importance_df = pd.DataFrame(list(importance_scores.items()), columns=['Feature', 'Importance'])\n",
        "\n",
        "    selected_features = importance_df[importance_df['Importance'] > threshold_importance]['Feature'].tolist()\n",
        "\n",
        "    feature_indices = [list(model.get_booster().feature_names).index(f) for f in selected_features]\n",
        "    X_train_selected = X_train.iloc[:, feature_indices]\n",
        "    X_test_selected = X_test.iloc[:, feature_indices]\n",
        "\n",
        "    key = f\"{dataset_name}_importanceScore\"\n",
        "    XGBoost_outliers_variants_features_selected[key] = (X_train_selected, X_test_selected, y_train, y_test)\n",
        "\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_variants.items():\n",
        "    model = XGBClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    importance_scores = model.get_booster().get_score(importance_type='gain')\n",
        "    importance_df = pd.DataFrame(list(importance_scores.items()), columns=['Feature', 'Importance'])\n",
        "    selected_features = importance_df[importance_df['Importance'] > threshold_importance]['Feature'].tolist()\n",
        "\n",
        "    feature_indices = [list(model.get_booster().feature_names).index(f) for f in selected_features]\n",
        "    X_train_filtered = X_train.iloc[:, feature_indices]\n",
        "    X_test_filtered = X_test.iloc[:, feature_indices]\n",
        "\n",
        "    estimator = XGBClassifier()\n",
        "    rfe = RFE(estimator, n_features_to_select=n_features_to_select)\n",
        "    rfe.fit(X_train_filtered, y_train)\n",
        "\n",
        "    rfe_selected_features_mask = X_train_filtered.columns[rfe.support_]\n",
        "\n",
        "    X_train_rfe = X_train_filtered.loc[:, rfe_selected_features_mask]\n",
        "    X_test_rfe = X_test_filtered.loc[:, rfe_selected_features_mask]\n",
        "\n",
        "    key = f\"{dataset_name}_rfeAfterImportanceFiltered\"\n",
        "    XGBoost_outliers_variants_features_selected[key] = (X_train_rfe, X_test_rfe, y_train, y_test)\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_variants.items():\n",
        "\n",
        "    estimator = XGBClassifier()\n",
        "    sfs = SequentialFeatureSelector(estimator, n_features_to_select=n_features_to_select, direction='forward', n_jobs=-1)\n",
        "    sfs.fit(X_train, y_train)\n",
        "\n",
        "    X_train_sfs = X_train.loc[:, sfs.get_support()]\n",
        "    X_test_sfs = X_test.loc[:, sfs.get_support()]\n",
        "\n",
        "    key = f\"{dataset_name}_sfsforward\"\n",
        "    XGBoost_outliers_variants_features_selected[key] = (X_train_sfs, X_test_sfs, y_train, y_test)\n",
        "\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_variants.items():\n",
        "    estimator = XGBClassifier(random_state=42)\n",
        "\n",
        "    rfecv = RFECV(\n",
        "        estimator=estimator,\n",
        "        step=1,\n",
        "        cv=cv,\n",
        "        scoring='accuracy',\n",
        "        min_features_to_select=1\n",
        "    )\n",
        "\n",
        "    rfecv.fit(X_train, y_train)\n",
        "\n",
        "    optimal_feature_count = rfecv.n_features_\n",
        "    feature_ranking = rfecv.ranking_\n",
        "    total_mean_score = np.mean(rfecv.cv_results_['mean_test_score'])\n",
        "\n",
        "    X_train_rfecv = X_train.loc[:, rfecv.support_]\n",
        "    X_test_rfecv = X_test.loc[:, rfecv.support_]\n",
        "\n",
        "    key = f\"{dataset_name}_rfecv\"\n",
        "    XGBoost_outliers_variants_features_selected[key] = (X_train_rfecv, X_test_rfecv, y_train, y_test)\n",
        "\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"Optimal number of features: {optimal_feature_count}\")\n",
        "    print(f\"Cross-validation scores for each iteration: {rfecv.cv_results_['mean_test_score']}\")\n",
        "    print(f\"Total mean score: {total_mean_score:.4f}\")\n",
        "\n",
        "\n",
        "print(XGBoost_outliers_variants_features_selected.keys())\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "KF1QfTiXtOHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imputed_variants.keys())\n",
        "print(feature_engineered_variants.keys())\n",
        "print(XGBoost_outliers_variants.keys())\n",
        "print(XGBoost_outliers_variants_features_selected.keys())"
      ],
      "metadata": {
        "id": "ZYNy5_JXkLw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_test, X_test_test, y_train_test, y_test_test = XGBoost_outliers_variants_features_selected['point7_Zscore_SVMSMOTE_importanceScore']\n",
        "\n",
        "data_check = pd.concat([X_train_test, y_train_test], axis=1)\n",
        "\n",
        "data_check.describe().T"
      ],
      "metadata": {
        "id": "eoEbUF7Aifqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_test, X_test_test, y_train_test, y_test_test = XGBoost_outliers_variants_features_selected['point7_IQR_SMOTETomek_rfecv']\n",
        "\n",
        "X_train_test = pd.DataFrame(X_train_test)\n",
        "\n",
        "y_train_test = y_train_test.reset_index(drop=True)\n",
        "\n",
        "data_check = pd.concat([X_train_test, y_train_test], axis=1)\n",
        "\n",
        "data_check.describe().T"
      ],
      "metadata": {
        "id": "PFjOOO_KYd2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_results = []\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_variants_features_selected.items():\n",
        "\n",
        "        smote_best_xgb.fit(X_train, y_train)\n",
        "        y_pred = smote_best_xgb.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        model_results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Variant\": dataset_name,\n",
        "            \"Feature Engineering\": dataset_name.split('_')[0],\n",
        "            \"Imputation Method\": dataset_name.split('_')[1],\n",
        "            \"Smote Method\": dataset_name.split('_')[2],\n",
        "            \"Features Selection Method\": dataset_name.split('_')[3],\n",
        "            \"Accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "model_results_df = pd.DataFrame(model_results)\n",
        "\n",
        "model_results_df = model_results_df.sort_values(by=\"Accuracy\", ascending=False)\n",
        "\n",
        "pd.set_option('display.max_rows', 10000)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 10000)\n",
        "print(model_results_df)"
      ],
      "metadata": {
        "id": "aZEC6Ju0vi8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "point7_Zscore_SMOTETomek_importanceScore 88%\n",
        "\n",
        "point7_IQR_SMOTETomek_importanceScore and  88%\n",
        "\n",
        "point7_IQR_SMOTETomek_rfecv 89% (have the best for now overall)\n",
        "\n",
        "point7_IQR_SMOTETomek_rfeAfterImportanceFiltered 89%"
      ],
      "metadata": {
        "id": "AjvFk4pPVbnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_Zscore_SMOTETomek_rfecv'\n",
        "X_train, X_test, y_train, y_test = XGBoost_outliers_variants_features_selected[best_variant_name]\n",
        "print(f\"Running model for variant: {best_variant_name}\")\n",
        "\n",
        "all_accuracies = []\n",
        "num_runs = 10\n",
        "\n",
        "for i in range(num_runs):\n",
        "    X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=i\n",
        "    )\n",
        "\n",
        "    print(f\"Features before fitting (run {i + 1}): {X_train_best.columns}\")\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        learning_rate=0.05,\n",
        "        max_depth=11,\n",
        "        n_estimators=200,\n",
        "        subsample=0.5,\n",
        "        eval_metric='logloss',\n",
        "        reg_lambda=1.5,\n",
        "        reg_alpha=0.5,\n",
        "        min_child_weight=10,\n",
        "        gamma=0.5,\n",
        "        colsample_bytree=1,\n",
        "        colsample_bynode=1,\n",
        "        colsample_bylevel=0.5,\n",
        "        booster='gbtree',\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_best, y_train_best)\n",
        "    y_pred = model.predict(X_test_best)\n",
        "\n",
        "    accuracy = accuracy_score(y_test_best, y_pred)\n",
        "    all_accuracies.append(accuracy)\n",
        "\n",
        "    scores = cross_validate(\n",
        "        model, X_train_best, y_train_best, cv=5, return_train_score=True, return_estimator=True\n",
        "    )\n",
        "\n",
        "    print(f\"Run {i + 1}:\")\n",
        "    print(f\"Accuracy (Testing): {accuracy:.2f}\")\n",
        "    print(f\"Accuracy (CV Mean): {np.mean(scores['test_score']):.2f} (+/- {np.std(scores['test_score']) * 2:.2f})\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_best, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSummary of accuracies across runs:\")\n",
        "print(f\"Mean accuracy over {num_runs} runs: {np.mean(all_accuracies):.2f} (+/- {np.std(all_accuracies):.2f})\")\n",
        "\n",
        "print(classification_report(y_test_best, y_pred))"
      ],
      "metadata": {
        "id": "4wYQE64QN-KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_runs_results = []\n",
        "\n",
        "num_runs = 10\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_variants_features_selected.items():\n",
        "\n",
        "    all_accuracies = []\n",
        "    for i in range(num_runs):\n",
        "\n",
        "        X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=i\n",
        "        )\n",
        "\n",
        "        print(f\"Features before fitting (run {i + 1}): {X_train_best.columns}\")\n",
        "        print(f\"Running model for variant: {dataset_name}\")\n",
        "\n",
        "        model = XGBClassifier(\n",
        "            learning_rate=0.05,\n",
        "            max_depth=10,\n",
        "            n_estimators=200,\n",
        "            subsample=0.8,\n",
        "            objective='reg:squarederror',\n",
        "            reg_lambda=1.5,\n",
        "            reg_alpha=1.5,\n",
        "            min_child_weight=10,\n",
        "            gamma=0.5,\n",
        "            colsample_bytree=1,\n",
        "            colsample_bynode=1,\n",
        "            colsample_bylevel=0.5,\n",
        "            booster='gbtree',\n",
        "            random_state=i\n",
        "        )\n",
        "\n",
        "        model.fit(X_train_best, y_train_best)\n",
        "\n",
        "        y_pred = model.predict(X_test_best)\n",
        "        accuracy = accuracy_score(y_test_best, y_pred)\n",
        "        all_accuracies.append(accuracy)\n",
        "\n",
        "        scores = cross_validate(\n",
        "            model, X_train_best, y_train_best, cv=5, return_train_score=True, return_estimator=True\n",
        "        )\n",
        "\n",
        "        print(f\"\\nRun {i + 1}:\")\n",
        "        print(f\"Accuracy (Testing): {accuracy:.2f}\")\n",
        "        print(f\"Accuracy (CV Mean): {np.mean(scores['test_score']):.2f} (+/- {np.std(scores['test_score']) * 2:.2f})\")\n",
        "\n",
        "    mean_accuracy = np.mean(all_accuracies)\n",
        "    std_accuracy = np.std(all_accuracies)\n",
        "    all_runs_results.append({\n",
        "        \"Dataset\": dataset_name,\n",
        "        \"Mean Accuracy\": mean_accuracy,\n",
        "        \"Std Accuracy\": std_accuracy,\n",
        "        \"Details\": X_train.columns.tolist()\n",
        "    })\n",
        "\n",
        "print(\"\\nSummary of accuracies across runs:\")\n",
        "for result in all_runs_results:\n",
        "    print(f\"Dataset: {result['Dataset']}, Mean Accuracy: {result['Mean Accuracy']:.2f} (+/- {result['Std Accuracy']:.2f})\")\n",
        "\n",
        "results_df = pd.DataFrame(all_runs_results)\n",
        "sorted_results_df = results_df.sort_values(by=\"Mean Accuracy\", ascending=False)\n",
        "\n",
        "print(\"\\nSorted Results by Accuracy:\")\n",
        "print(sorted_results_df)\n",
        "\n",
        "import ace_tools as tools\n",
        "tools.display_dataframe_to_user(name=\"Sorted Model Results by Accuracy\", dataframe=sorted_results_df)"
      ],
      "metadata": {
        "id": "LcXnpg53qqFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### output model and safe work state"
      ],
      "metadata": {
        "id": "VGVuNU4PmY-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_Zscore_SMOTETomek_rfecv'\n",
        "XGB_finial_X_train, X_test, XGB_finial_y_train, y_test = XGBoost_outliers_variants_features_selected[best_variant_name]  # unpack the value\n",
        "\n",
        "best_XGB_After_proccess = XGBClassifier(\n",
        "        learning_rate=0.05,\n",
        "        max_depth=11,\n",
        "        n_estimators=200,\n",
        "        subsample=0.5,\n",
        "        eval_metric='logloss',\n",
        "        reg_lambda=1.5,\n",
        "        reg_alpha=0.5,\n",
        "        min_child_weight=10,\n",
        "        gamma=0.5,\n",
        "        colsample_bytree=1,\n",
        "        colsample_bynode=1,\n",
        "        colsample_bylevel=0.5,\n",
        "        booster='gbtree',\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "best_XGB_After_proccess.fit(XGB_finial_X_train, XGB_finial_y_train)"
      ],
      "metadata": {
        "id": "DTpV9JkT4GME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(best_XGB_After_proccess, 'best_xgboost_model.pkl')"
      ],
      "metadata": {
        "id": "V7lm-Pp-ljUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_XGB_After_process = joblib.load('best_xgboost_model.pkl')\n",
        "y_pred = best_XGB_After_process.predict(X_test)"
      ],
      "metadata": {
        "id": "9HWycHc5l6yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_XGB_After_process.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "tY8h8d8ol-Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(best_XGB_After_process, 'best_xgboost_model.pkl')"
      ],
      "metadata": {
        "id": "68Xtaj6gmo9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imputed_variants.keys())\n",
        "print(feature_engineered_variants.keys())\n",
        "print(XGBoost_outliers_variants.keys())\n",
        "print(XGBoost_outliers_variants_features_selected.keys())"
      ],
      "metadata": {
        "id": "Y-F0iwxS7Ivf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning for Gradient Boosting (kaggle 0.8505)  "
      ],
      "metadata": {
        "id": "h5WimoAOeuqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "\n",
        "best_variant_name = 'point7_IQR_SMOTETomek_rfecv'\n",
        "X_train, X_test, y_train, y_test = XGBoost_outliers_variants_features_selected[best_variant_name]\n",
        "\n",
        "param_space = {\n",
        "    'n_estimators': (50, 100, 200, 300),\n",
        "    'learning_rate': (0.01, 0.05, 0.1, 0.2),\n",
        "    'max_depth': (3, 5, 7),\n",
        "    'min_samples_split': (2, 5, 10),\n",
        "    'min_samples_leaf': (1, 2, 5, 10),\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'subsample': (0.7, 0.8, 1.0),\n",
        "    'loss': ['log_loss', 'exponential'],\n",
        "    'min_impurity_decrease': (0.001, 0.01, 0.1),\n",
        "    'warm_start': [True, False],\n",
        "    'max_leaf_nodes': [None, 10, 20, 30, 50],\n",
        "    'n_iter_no_change': [None, 5, 10, 15],\n",
        "    'tol': (0.0001, 0.001, 0.01)\n",
        "}\n",
        "\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "bayes_opt = BayesSearchCV(\n",
        "    estimator=model,\n",
        "    search_spaces=param_space,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bayes_opt.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters: \", bayes_opt.best_params_)\n",
        "print(\"Best Accuracy from Grid Search: \", bayes_opt.best_score_)"
      ],
      "metadata": {
        "id": "Xd5KXIIl-C4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_IQR_SMOTETomek_rfecv'\n",
        "X_train, X_test, y_train, y_test = XGBoost_outliers_variants_features_selected[best_variant_name]\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'subsample': [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "\n",
        "gb_model = GradientBoostingClassifier()\n",
        "\n",
        "grid_search = GridSearchCV(estimator=gb_model,\n",
        "                           param_grid=param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=3,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=2)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "CkY6dD7DeuT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_runs_results = []\n",
        "\n",
        "num_runs = 10\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_variants_features_selected.items():\n",
        "\n",
        "    all_accuracies = []\n",
        "    for i in range(num_runs):\n",
        "\n",
        "        X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=i\n",
        "        )\n",
        "\n",
        "        print(f\"Features before fitting (run {i + 1}): {X_train_best.columns}\")\n",
        "        print(f\"Running model for variant: {dataset_name}\")\n",
        "\n",
        "        model = GradientBoostingClassifier(\n",
        "            learning_rate=0.2,\n",
        "            max_depth=7,\n",
        "            n_estimators=50,\n",
        "            subsample=1.0,\n",
        "            max_features='log2',\n",
        "            min_samples_leaf=1,\n",
        "            min_samples_split=10,\n",
        "            random_state=42,\n",
        "            warm_start=False,\n",
        "            tol=0.001,\n",
        "            min_impurity_decrease=0.001,\n",
        "            max_leaf_nodes=None,\n",
        "            loss='exponential',\n",
        "            n_iter_no_change=None\n",
        "        )\n",
        "\n",
        "        model.fit(X_train_best, y_train_best)\n",
        "\n",
        "        y_pred = model.predict(X_test_best)\n",
        "        accuracy = accuracy_score(y_test_best, y_pred)\n",
        "        all_accuracies.append(accuracy)\n",
        "\n",
        "        scores = cross_validate(\n",
        "            model, X_train_best, y_train_best, cv=5, return_train_score=True, return_estimator=True\n",
        "        )\n",
        "\n",
        "        print(f\"\\nRun {i + 1}:\")\n",
        "        print(f\"Accuracy (Testing): {accuracy:.2f}\")\n",
        "        print(f\"Accuracy (CV Mean): {np.mean(scores['test_score']):.2f} (+/- {np.std(scores['test_score']) * 2:.2f})\")\n",
        "\n",
        "    mean_accuracy = np.mean(all_accuracies)\n",
        "    std_accuracy = np.std(all_accuracies)\n",
        "    all_runs_results.append({\n",
        "        \"Dataset\": dataset_name,\n",
        "        \"Mean Accuracy\": mean_accuracy,\n",
        "        \"Std Accuracy\": std_accuracy,\n",
        "        \"Details\": X_train.columns.tolist()\n",
        "    })\n",
        "\n",
        "print(\"\\nSummary of accuracies across runs:\")\n",
        "for result in all_runs_results:\n",
        "    print(f\"Dataset: {result['Dataset']}, Mean Accuracy: {result['Mean Accuracy']:.2f} (+/- {result['Std Accuracy']:.2f})\")\n",
        "\n",
        "results_df = pd.DataFrame(all_runs_results)\n",
        "sorted_results_df = results_df.sort_values(by=\"Mean Accuracy\", ascending=False)\n",
        "\n",
        "print(\"\\nSorted Results by Accuracy:\")\n",
        "print(sorted_results_df)\n",
        "\n",
        "import ace_tools as tools\n",
        "tools.display_dataframe_to_user(name=\"Sorted Model Results by Accuracy\", dataframe=sorted_results_df)"
      ],
      "metadata": {
        "id": "EgCQdPFgBEhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_Zscore_SMOTETomek_rfecv'\n",
        "X_train, X_test, y_train, y_test = XGBoost_outliers_variants_features_selected[best_variant_name]\n",
        "print(f\"Running model for variant: {best_variant_name}\")\n",
        "\n",
        "all_accuracies = []\n",
        "num_runs = 10\n",
        "\n",
        "for i in range(num_runs):\n",
        "    X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=i\n",
        "    )\n",
        "\n",
        "    print(f\"Features before fitting (run {i + 1}): {X_train_best.columns}\")\n",
        "\n",
        "    model = GradientBoostingClassifier(\n",
        "        learning_rate=0.2,\n",
        "        max_depth=7,\n",
        "        n_estimators=50,\n",
        "        subsample=1.0,\n",
        "        max_features='log2',\n",
        "        min_samples_leaf=1,\n",
        "        min_samples_split=10,\n",
        "        random_state=42,\n",
        "        warm_start=False,\n",
        "        tol=0.001,\n",
        "        min_impurity_decrease=0.001,\n",
        "        max_leaf_nodes=None,\n",
        "        loss='exponential',\n",
        "        n_iter_no_change=None\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_best, y_train_best)\n",
        "    y_pred = model.predict(X_test_best)\n",
        "\n",
        "    accuracy = accuracy_score(y_test_best, y_pred)\n",
        "    all_accuracies.append(accuracy)\n",
        "\n",
        "    scores = cross_validate(\n",
        "        model, X_train_best, y_train_best, cv=5, return_train_score=True, return_estimator=True\n",
        "    )\n",
        "\n",
        "    print(f\"Run {i + 1}:\")\n",
        "    print(f\"Accuracy (Testing): {accuracy:.2f}\")\n",
        "    print(f\"Accuracy (CV Mean): {np.mean(scores['test_score']):.2f} (+/- {np.std(scores['test_score']) * 2:.2f})\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_best, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSummary of accuracies across runs:\")\n",
        "print(f\"Mean accuracy over {num_runs} runs: {np.mean(all_accuracies):.2f} (+/- {np.std(all_accuracies):.2f})\")\n",
        "\n",
        "print(classification_report(y_test_best, y_pred))"
      ],
      "metadata": {
        "id": "ayX2Itnj_DLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_Zscore_SMOTETomek_rfecv'\n",
        "GB_finial_X_train, X_test, GB_finial_y_train, y_test = XGBoost_outliers_variants_features_selected[best_variant_name]\n",
        "\n",
        "best_GB_After_proccess = GradientBoostingClassifier(\n",
        "        learning_rate=0.2,\n",
        "        max_depth=7,\n",
        "        n_estimators=50,\n",
        "        subsample=1.0,\n",
        "        max_features='log2',\n",
        "        min_samples_leaf=1,\n",
        "        min_samples_split=10,\n",
        "        random_state=42,\n",
        "        warm_start=False,\n",
        "        tol=0.001,\n",
        "        min_impurity_decrease=0.001,\n",
        "        max_leaf_nodes=None,\n",
        "        loss='exponential',\n",
        "        n_iter_no_change=None\n",
        "    )\n",
        "\n",
        "best_GB_After_proccess.fit(GB_finial_X_train, GB_finial_y_train)"
      ],
      "metadata": {
        "id": "B9xaiAuIiyUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(best_GB_After_proccess, 'best_GB_After_proccess.pkl')"
      ],
      "metadata": {
        "id": "wZE8OFSiNXCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_XGB_After_process = joblib.load('best_xgboost_model.pkl')\n",
        "y_pred = best_XGB_After_process.predict(X_test)"
      ],
      "metadata": {
        "id": "6sfILtVlOGNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_XGB_After_process.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "PMTors3gOJt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_gb_model.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "LQL6JvveqKL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gradient Boosting play ground"
      ],
      "metadata": {
        "id": "c_fbRmNJy8vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = p7_point_footprints_df_iqr_knn_blance.drop('sex', axis=1)\n",
        "y = p7_point_footprints_df_iqr_knn_blance['sex']\n",
        "x = 0\n",
        "count = 0\n",
        "num_runs = 1\n",
        "\n",
        "for x in range (num_runs):\n",
        "\n",
        "    count += 1\n",
        "    model = GradientBoostingClassifier(n_estimators=150,\n",
        "                                       learning_rate=0.5,\n",
        "                                       max_depth=7,\n",
        "                                       min_samples_split=5,\n",
        "                                       min_samples_leaf=2,)\n",
        "\n",
        "\n",
        "    SMOTE_iso_X_train, SMOTE_iso_X_test, SMOTE_iso_y_train, SMOTE_iso_y_test = train_test_split(X,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=1,\n",
        "                                                       )\n",
        "\n",
        "    model.fit(SMOTE_iso_X_train, SMOTE_iso_y_train)\n",
        "    SMOTE_iso_y_pred = model.predict(SMOTE_iso_X_test)\n",
        "\n",
        "scores = cross_validate(model, X, y, cv=3, return_train_score=True, return_estimator=True)\n",
        "precision = precision_score(SMOTE_iso_y_test, SMOTE_iso_y_pred)\n",
        "recall = recall_score(SMOTE_iso_y_test, SMOTE_iso_y_pred)\n",
        "\n",
        "print(metrics.confusion_matrix(SMOTE_iso_y_test, SMOTE_iso_y_pred))\n",
        "print(\"\\nAccuracy (Testing):  %0.2f \" % (metrics.accuracy_score(SMOTE_iso_y_test, SMOTE_iso_y_pred)))\n",
        "print(\"Accuracy (Testing):  %0.2f (+/- %0.2f)\" % (scores['test_score'].mean(), scores['test_score'].std() * 2))\n",
        "print(\"count:\" , count)\n",
        "print(\"Precision: %.2f\" % precision)\n",
        "print(\"recall: %.2f\" % recall)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(SMOTE_iso_y_test,SMOTE_iso_y_pred))\n",
        "sns.heatmap(confusion_matrix(SMOTE_iso_y_test,SMOTE_iso_y_pred),annot=True)"
      ],
      "metadata": {
        "id": "8hjJTQbFzHdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SMOTE_iso_best_gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        ")\n",
        "\n",
        "SMOTE_iso_best_gb_model.fit(SMOTE_iso_X_train, SMOTE_iso_y_train)"
      ],
      "metadata": {
        "id": "oZO4o6uuCUtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning for Support Vector Machines"
      ],
      "metadata": {
        "id": "H17teZK_xW3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM overall will do better after StandardScaler there for we will use it to improve SVM score"
      ],
      "metadata": {
        "id": "V2Kz0Iz7jETr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "best_variant_name = 'point7_IQR_SMOTETomek_rfecv'\n",
        "X_train, X_test, y_train, y_test = XGBoost_outliers_variants_features_selected[best_variant_name]\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "param_grid = {\n",
        "    'C': (0.1, 1000, 'log-uniform'),\n",
        "    'gamma': (0.001, 10, 'log-uniform'),\n",
        "    'kernel': ['rbf'],\n",
        "    'tol': (1e-4, 1e-2, 'log-uniform'),\n",
        "    'max_iter': (1000, 10000),\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "model = SVC(random_state=42)\n",
        "\n",
        "bayes_opt = BayesSearchCV(\n",
        "    estimator=model,\n",
        "    search_spaces=param_grid,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bayes_opt.fit(X_train_pca, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", bayes_opt.best_params_)\n",
        "print(\"Best Score:\", bayes_opt.best_score_)"
      ],
      "metadata": {
        "id": "gyAZBDz1xWFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DQjAAGoDjBpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "XGBoost_outliers_features_selected_scaled_variants = {}\n",
        "\n",
        "scalers = {\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'RobustScaler': RobustScaler()\n",
        "}\n",
        "\n",
        "for scaler_name, scaler in scalers.items():\n",
        "    for variant_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_variants_features_selected.items():\n",
        "\n",
        "        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "        key = f\"{variant_name}_{scaler_name}\"\n",
        "        XGBoost_outliers_features_selected_scaled_variants[key] = (X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "print(XGBoost_outliers_features_selected_scaled_variants.keys())"
      ],
      "metadata": {
        "id": "CNBN_KvPjY4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_test, X_test_test, y_train_test, y_test_test = XGBoost_outliers_features_selected_scaled_variants['point7_Zscore_SMOTETomek_rfecv_StandardScaler']\n",
        "\n",
        "X_train_test = pd.DataFrame(X_train_test)\n",
        "\n",
        "y_train_test = y_train_test.reset_index(drop=True)\n",
        "\n",
        "data_check = pd.concat([X_train_test, y_train_test], axis=1)\n",
        "\n",
        "data_check.describe().T"
      ],
      "metadata": {
        "id": "uysCamM2ij1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_test, X_test_test, y_train_test, y_test_test = XGBoost_outliers_variants_features_selected['point7_Zscore_SMOTETomek_rfecv']\n",
        "\n",
        "X_train_test = pd.DataFrame(X_train_test)\n",
        "\n",
        "y_train_test = y_train_test.reset_index(drop=True)\n",
        "\n",
        "data_check = pd.concat([X_train_test, y_train_test], axis=1)\n",
        "\n",
        "data_check.describe().T"
      ],
      "metadata": {
        "id": "yqqIOznuitVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_runs_results = []\n",
        "\n",
        "num_runs = 10\n",
        "\n",
        "best_params = {\n",
        "    'C': 1.1930801848463657,\n",
        "    'class_weight': 'balanced',\n",
        "    'gamma': 0.120601154417892,\n",
        "    'kernel': 'rbf',\n",
        "    'max_iter': 1000,\n",
        "    'tol': 0.0001\n",
        "}\n",
        "\n",
        "for dataset_name, (X_train, X_test, y_train, y_test) in XGBoost_outliers_features_selected_scaled_variants.items():\n",
        "\n",
        "    all_accuracies = []\n",
        "    for i in range(num_runs):\n",
        "\n",
        "        X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=i\n",
        "        )\n",
        "\n",
        "        print(f\"Features before fitting (run {i + 1}): {X_train_best.columns}\")\n",
        "        print(f\"Running model for variant: {dataset_name}\")\n",
        "\n",
        "        model = SVC(\n",
        "            C=best_params['C'],\n",
        "            kernel=best_params['kernel'],\n",
        "            gamma=best_params['gamma'],\n",
        "            class_weight=best_params['class_weight'],\n",
        "            max_iter=best_params['max_iter'],\n",
        "            tol=best_params['tol'],\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        model.fit(X_train_best, y_train_best)\n",
        "\n",
        "        y_pred = model.predict(X_test_best)\n",
        "        accuracy = accuracy_score(y_test_best, y_pred)\n",
        "        all_accuracies.append(accuracy)\n",
        "\n",
        "        scores = cross_validate(\n",
        "            model, X_train_best, y_train_best, cv=5, return_train_score=True, return_estimator=True\n",
        "        )\n",
        "\n",
        "        print(f\"\\nRun {i + 1}:\")\n",
        "        print(f\"Accuracy (Testing): {accuracy:.2f}\")\n",
        "        print(f\"Accuracy (CV Mean): {np.mean(scores['test_score']):.2f} (+/- {np.std(scores['test_score']) * 2:.2f})\")\n",
        "\n",
        "    mean_accuracy = np.mean(all_accuracies)\n",
        "    std_accuracy = np.std(all_accuracies)\n",
        "    all_runs_results.append({\n",
        "        \"Dataset\": dataset_name,\n",
        "        \"Mean Accuracy\": mean_accuracy,\n",
        "        \"Std Accuracy\": std_accuracy,\n",
        "        \"Details\": X_train.columns.tolist()\n",
        "    })\n",
        "\n",
        "print(\"\\nSummary of accuracies across runs:\")\n",
        "for result in all_runs_results:\n",
        "    print(f\"Dataset: {result['Dataset']}, Mean Accuracy: {result['Mean Accuracy']:.2f} (+/- {result['Std Accuracy']:.2f})\")\n",
        "\n",
        "results_df = pd.DataFrame(all_runs_results)\n",
        "sorted_results_df = results_df.sort_values(by=\"Mean Accuracy\", ascending=False)\n",
        "\n",
        "print(\"\\nSorted Results by Accuracy:\")\n",
        "print(sorted_results_df)\n",
        "\n",
        "tools.display_dataframe_to_user(name=\"Sorted Results by Accuracy\", dataframe=sorted_results_df)"
      ],
      "metadata": {
        "id": "igXErT-DiD0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_Zscore_SMOTETomek_rfecv_StandardScaler'\n",
        "X_train, X_test, y_train, y_test = XGBoost_outliers_features_selected_scaled_variants[best_variant_name]\n",
        "print(f\"Running model for variant: {best_variant_name}\")\n",
        "\n",
        "all_accuracies = []\n",
        "num_runs = 10\n",
        "\n",
        "for i in range(num_runs):\n",
        "    X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=i\n",
        "    )\n",
        "\n",
        "    print(f\"Features before fitting (run {i + 1}): {X_train_best.columns}\")\n",
        "\n",
        "    model = SVC(\n",
        "        C =1.9650743261576813,\n",
        "        class_weight = 'balanced',\n",
        "        gamma = 0.09007054559274681,\n",
        "        kernel = 'rbf',\n",
        "        max_iter = 4979,\n",
        "        tol = 0.01,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_best, y_train_best)\n",
        "    y_pred = model.predict(X_test_best)\n",
        "\n",
        "    accuracy = accuracy_score(y_test_best, y_pred)\n",
        "    all_accuracies.append(accuracy)\n",
        "\n",
        "    scores = cross_validate(\n",
        "        model, X_train_best, y_train_best, cv=5, return_train_score=True, return_estimator=True\n",
        "    )\n",
        "\n",
        "    print(f\"Run {i + 1}:\")\n",
        "    print(f\"Accuracy (Testing): {accuracy:.2f}\")\n",
        "    print(f\"Accuracy (CV Mean): {np.mean(scores['test_score']):.2f} (+/- {np.std(scores['test_score']) * 2:.2f})\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_best, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSummary of accuracies across runs:\")\n",
        "print(f\"Mean accuracy over {num_runs} runs: {np.mean(all_accuracies):.2f} (+/- {np.std(all_accuracies):.2f})\")\n",
        "\n",
        "print(classification_report(y_test_best, y_pred))"
      ],
      "metadata": {
        "id": "9TEgXO4gqtjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Support Vector Machines play ground"
      ],
      "metadata": {
        "id": "4Sdy_gQ-JFFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_Zscore_SMOTETomek_rfecv_StandardScaler'\n",
        "SCV_finial_X_train, X_test, SCV_finial_y_train, y_test = XGBoost_outliers_features_selected_scaled_variants[best_variant_name]\n",
        "\n",
        "best_SVC_After_proccess = SVC(\n",
        "        C =1.9650743261576813,\n",
        "        class_weight = 'balanced',\n",
        "        gamma = 0.09007054559274681,\n",
        "        kernel = 'rbf',\n",
        "        max_iter = 4979,\n",
        "        tol = 0.01,\n",
        "        random_state=42,\n",
        "        probability=True\n",
        "    )\n",
        "\n",
        "best_SVC_After_proccess.fit(SCV_finial_X_train, SCV_finial_y_train)"
      ],
      "metadata": {
        "id": "1WuOsXPtKSGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(best_SVC_After_proccess, 'best_SVC_After_proccess.pkl')"
      ],
      "metadata": {
        "id": "YQgLRG8UuxQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_SVC_After_proccess = joblib.load('best_SVC_After_proccess.pkl')\n",
        "y_pred = best_SVC_After_proccess.predict(X_test)"
      ],
      "metadata": {
        "id": "97voPxd3u02s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "id": "WS4rTVofu6y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning for Random Forest"
      ],
      "metadata": {
        "id": "hPw38KqCuDVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_Zscore_SMOTETomek_rfecv_StandardScaler'\n",
        "X_train, X_test, y_train, y_test = XGBoost_outliers_features_selected_scaled_variants[best_variant_name]\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 500],\n",
        "    'max_depth': [5, 10, 15, 20, None],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'max_features': ['sqrt', 'log2', None, 0.5],\n",
        "    'bootstrap': [True],\n",
        "    'max_leaf_nodes': [10, 20, 50, 100, None],\n",
        "    'min_impurity_decrease': [0.0, 0.001, 0.01],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'class_weight': [None, 'balanced', 'balanced_subsample'],\n",
        "    'oob_score': [True, False]\n",
        "}\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "bayes_opt = BayesSearchCV(\n",
        "    estimator=model,\n",
        "    search_spaces=param_grid,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bayes_opt.fit(X_train_pca, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", bayes_opt.best_params_)\n",
        "print(\"Best Score:\", bayes_opt.best_score_)"
      ],
      "metadata": {
        "id": "QJ60G3IWwOtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_Zscore_SMOTETomek_rfecv_StandardScaler'\n",
        "X_train, X_test, y_train, y_test = XGBoost_outliers_features_selected_scaled_variants[best_variant_name]\n",
        "print(f\"Running model for variant: {best_variant_name}\")\n",
        "\n",
        "all_accuracies = []\n",
        "num_runs = 10\n",
        "\n",
        "for i in range(num_runs):\n",
        "    X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=i\n",
        "    )\n",
        "\n",
        "    print(f\"Features before fitting (run {i + 1}): {X_train_best.columns}\")\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        class_weight =None,\n",
        "        criterion = 'entropy',\n",
        "        max_depth = None,\n",
        "        max_features = None,\n",
        "        max_leaf_nodes = 100,\n",
        "        min_impurity_decrease = 0.001,\n",
        "        min_samples_leaf = 2,\n",
        "        min_samples_split = 2,\n",
        "        n_estimators = 200,\n",
        "        oob_score = False,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_best, y_train_best)\n",
        "    y_pred = model.predict(X_test_best)\n",
        "\n",
        "    accuracy = accuracy_score(y_test_best, y_pred)\n",
        "    all_accuracies.append(accuracy)\n",
        "\n",
        "    scores = cross_validate(\n",
        "        model, X_train_best, y_train_best, cv=5, return_train_score=True, return_estimator=True\n",
        "    )\n",
        "\n",
        "    print(f\"Run {i + 1}:\")\n",
        "    print(f\"Accuracy (Testing): {accuracy:.2f}\")\n",
        "    print(f\"Accuracy (CV Mean): {np.mean(scores['test_score']):.2f} (+/- {np.std(scores['test_score']) * 2:.2f})\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_best, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSummary of accuracies across runs:\")\n",
        "print(f\"Mean accuracy over {num_runs} runs: {np.mean(all_accuracies):.2f} (+/- {np.std(all_accuracies):.2f})\")\n",
        "\n",
        "print(classification_report(y_test_best, y_pred))"
      ],
      "metadata": {
        "id": "gpfqv8IVyLi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_variant_name = 'point7_Zscore_SMOTETomek_rfecv_StandardScaler'\n",
        "RF_finial_X_train, X_test, RF_finial_y_train, y_test = XGBoost_outliers_features_selected_scaled_variants[best_variant_name]  # unpack the value\n",
        "\n",
        "best_RF_After_proccess = RandomForestClassifier(\n",
        "        class_weight =None,\n",
        "        criterion = 'entropy',\n",
        "        max_depth = None,\n",
        "        max_features = None,\n",
        "        max_leaf_nodes = 100,\n",
        "        min_impurity_decrease = 0.001,\n",
        "        min_samples_leaf = 2,\n",
        "        min_samples_split = 2,\n",
        "        n_estimators = 200,\n",
        "        oob_score = False,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "best_RF_After_proccess.fit(RF_finial_X_train, RF_finial_y_train)"
      ],
      "metadata": {
        "id": "Cc4R1Ypp1YiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##step 8: Ensemble Learning"
      ],
      "metadata": {
        "id": "y1RXOLUwveqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_model = XGBClassifier(best_XGB_After_proccess\n",
        "GBC_model = GradientBoostingClassifier(best_GB_After_proccess)\n",
        "SVM_model = SVC(best_SVC_After_proccess, probability=True)\n",
        "RF_model = RandomForestClassifier(best_RF_After_proccess)"
      ],
      "metadata": {
        "id": "nSyWf-Tc6yuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', best_XGB_After_proccess),\n",
        "        ('gbc', best_GB_After_proccess),\n",
        "        ('svm', best_SVC_After_proccess),\n",
        "        ('rf', best_RF_After_proccess)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "print(\"Ensemble Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "T3-2oNXu7XY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imputed_variants.keys())\n",
        "print(feature_engineered_variants.keys())\n",
        "print(XGBoost_outliers_variants.keys())\n",
        "print(XGBoost_outliers_features_selected_scaled_variants.keys())\n",
        "print(XGBoost_outliers_variants_features_selected.keys())"
      ],
      "metadata": {
        "id": "neqDrv5p_z2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGBoost_outliers_variants = {'point7_IQR_SMOTE': (X_train, X_test, y_train, y_test)}"
      ],
      "metadata": {
        "id": "q_VK4xqQ_ZgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', best_XGB_After_process),\n",
        "        ('gbc', best_GB_After_proccess),\n",
        "        ('svm', best_SVC_After_proccess),\n",
        "        ('rf', best_RF_After_proccess)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "print(\"Stacking Ensemble Accuracy:\", accuracy_score(y_test, y_pred_stack))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_stack))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_stack))"
      ],
      "metadata": {
        "id": "hoDvSpm49K5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe().T"
      ],
      "metadata": {
        "id": "M4_Pm59D-YKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', best_xgb),\n",
        "        ('gb', best_gb_model),\n",
        "\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "\n",
        "accuracy_ensemble = metrics.accuracy_score(y_test, y_pred_ensemble)\n",
        "print(\"Ensemble Model Accuracy:\", accuracy_ensemble)\n",
        "\n",
        "print(\"Confusion Matrix:\", metrics.confusion_matrix(y_test, y_pred_ensemble))\n",
        "print(\"Classification Report:\", metrics.classification_report(y_test, y_pred_ensemble))"
      ],
      "metadata": {
        "id": "ufllx3C0zXAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_ensemble_Voting_model =VotingClassifier(\n",
        "          estimators=[\n",
        "            ('xgb', best_XGB_After_proccess),\n",
        "            ('gbc', best_GB_After_proccess),\n",
        "            ('svm', best_SVC_After_proccess),\n",
        "            ('rf', best_RF_After_proccess)\n",
        "          ],\n",
        "          voting='soft'\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "# Assume X_train and y_train are the full training data (after preprocessing and scaling)\n",
        "best_ensemble_Voting_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gpgBeaYZQY2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "base_models = [\n",
        "      ('xgb', best_XGB_After_proccess),\n",
        "      ('gbc', best_GB_After_proccess),\n",
        "      ('svm', best_SVC_After_proccess),\n",
        "      ('rf', best_RF_After_proccess)\n",
        "]\n",
        "\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
        "stacking_model.fit(X_train, y_train)\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "accuracy_ensemble = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Ensemble Model Accuracy:\", accuracy_ensemble)\n",
        "\n",
        "print(\"Confusion Matrix:\", metrics.confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\", metrics.classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "2XJygeHVNZKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_ensemble_Stacking_model =StackingClassifier(\n",
        "          estimators=[\n",
        "            ('xgb', best_XGB_After_proccess),\n",
        "            ('gbc', best_GB_After_proccess),\n",
        "            ('svm', best_SVC_After_proccess),\n",
        "            ('rf', best_RF_After_proccess)\n",
        "          ],\n",
        "      )\n",
        "\n",
        "best_ensemble_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ojqEoUmfRET3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hold_out set change"
      ],
      "metadata": {
        "id": "QQqqpNp-KQGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hold_out = pd.read_csv('SexLandmarks-test.csv')"
      ],
      "metadata": {
        "id": "NjYFwcTnQOCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hold_out_data_df = hold_out.copy()"
      ],
      "metadata": {
        "id": "KEOME-V0VjOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hold_out_data_df = IQR(hold_out_data_df)"
      ],
      "metadata": {
        "id": "aTGBgBtDuCa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hold_out_scaled_data = hold_out.copy()\n",
        "\n",
        "for column in hold_out_scaled_data.columns:\n",
        "    if column.startswith('x'):\n",
        "        hold_out_scaled_data[column] = hold_out_scaled_data[column] * width\n",
        "    elif column.startswith('y'):\n",
        "        hold_out_scaled_data[column] = hold_out_scaled_data[column] * height\n",
        "\n",
        "print(hold_out_scaled_data.head())"
      ],
      "metadata": {
        "id": "_YqlfkFMQngx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths_upper_threshold = hold_out_scaled_data_with_lengths_widths['lengths'].quantile(0.95)\n",
        "lengths_lower_threshold = hold_out_scaled_data_with_lengths_widths['lengths'].quantile(0.05)\n",
        "widths_upper_threshold = hold_out_scaled_data_with_lengths_widths['widths'].quantile(0.95)\n",
        "widths_lower_threshold = hold_out_scaled_data_with_lengths_widths['widths'].quantile(0.05)\n",
        "\n",
        "big_feet = hold_out_scaled_data_with_lengths_widths[\n",
        "    (hold_out_scaled_data_with_lengths_widths['lengths'] > lengths_upper_threshold) |\n",
        "    (hold_out_scaled_data_with_lengths_widths['widths'] > widths_upper_threshold)\n",
        "]\n",
        "\n",
        "small_feet = hold_out_scaled_data_with_lengths_widths[\n",
        "    (hold_out_scaled_data_with_lengths_widths['lengths'] < lengths_lower_threshold) |\n",
        "    (hold_out_scaled_data_with_lengths_widths['widths'] < widths_lower_threshold)\n",
        "]\n",
        "\n",
        "print(\"Big Feet Data Points:\")\n",
        "print(big_feet)\n",
        "\n",
        "print(\"\\nSmall Feet Data Points:\")\n",
        "print(small_feet)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.scatter(hold_out_scaled_data_with_lengths_widths['lengths'], hold_out_scaled_data_with_lengths_widths['widths'], alpha=0.5, label='Normal Data')\n",
        "\n",
        "plt.scatter(big_feet['lengths'], big_feet['widths'], color='red', label='Big Feet Outliers', edgecolor='black')\n",
        "plt.scatter(small_feet['lengths'], small_feet['widths'], color='yellow', label='Small Feet Outliers', edgecolor='black')\n",
        "\n",
        "plt.xlabel('Lengths')\n",
        "plt.ylabel('Widths')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KQtg_frTRFF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_foot_1 = big_feet[\n",
        "    (big_feet['lengths'] > 2200) & (big_feet['lengths'] < 2300) &\n",
        "    (big_feet['widths'] > 1000) & (big_feet['widths'] < 1100)\n",
        "]"
      ],
      "metadata": {
        "id": "L9YHbQEnRU0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not small_foot_1.empty:\n",
        "    plot_footprint(small_foot_1.iloc[0], 'Visual Representation of Small Foot 1 (Length ~ 1780, Width ~ 200)')"
      ],
      "metadata": {
        "id": "rdxekx5eReqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holdout_datasets = {\n",
        "    'IQR': IQR(hold_out_data_df),\n",
        "    'RobustScaling': cap_outliers_and_scale(hold_out_data_df),\n",
        "    'Winsorization': Winsorization(hold_out_data_df),\n",
        "    'Zscore': z_score(hold_out_data_df),\n",
        "}\n",
        "\n",
        "imputed_variants_holdout = {}\n",
        "\n",
        "for variant_name, hold_out_data in holdout_datasets.items():\n",
        "    hold_out_imputed = pd.DataFrame(IterativeImputer.fit_transform(hold_out_data), columns=hold_out_data.columns)\n",
        "    key = f\"{variant_name}_Iterative\"\n",
        "    imputed_variants_holdout[key] = hold_out_imputed\n",
        "\n",
        "for variant_name, hold_out_data in holdout_datasets.items():\n",
        "    hold_out_imputed = pd.DataFrame(KNNImputer.fit_transform(hold_out_data), columns=hold_out_data.columns)\n",
        "    key = f\"{variant_name}_knn\"\n",
        "    imputed_variants_holdout[key] = hold_out_imputed\n",
        "\n",
        "print(imputed_variants_holdout.keys())"
      ],
      "metadata": {
        "id": "ApEN6hJnUTeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holdout_feature_engineered_variants = {}\n",
        "\n",
        "lengths_widths_temp_dict = {}\n",
        "\n",
        "for variant_name, hold_out_data in imputed_variants_holdout.items():\n",
        "\n",
        "    hold_out_lengths = lengths_widths_calculation(hold_out_data)\n",
        "\n",
        "    key = f\"{variant_name}_lengths_widths\"\n",
        "\n",
        "    lengths_widths_temp_dict[key] = hold_out_lengths\n",
        "\n",
        "holdout_feature_engineered_variants.update(lengths_widths_temp_dict)\n",
        "\n",
        "point7_temp_dict = {}\n",
        "\n",
        "for variant_name, hold_out_data in imputed_variants_holdout.items():\n",
        "\n",
        "    hold_out_point7 = point7_calculation(hold_out_data)\n",
        "    key = f\"{variant_name}_point7\"\n",
        "    point7_temp_dict[key] = hold_out_point7\n",
        "\n",
        "holdout_feature_engineered_variants.update(point7_temp_dict)\n",
        "\n",
        "print(holdout_feature_engineered_variants.keys())"
      ],
      "metadata": {
        "id": "agmA7pihWBPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(holdout_feature_engineered_variants.keys())\n",
        "print(imputed_variants_holdout.keys())"
      ],
      "metadata": {
        "id": "38FlD22JEnh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hold_out_XGBoost_outliers_variants = {}\n",
        "\n",
        "hold_out_data = holdout_feature_engineered_variants['Winsorization_Iterative_point7']\n",
        "\n",
        "hold_out_point7_datasets = {\n",
        "    'point7_IQR': (IQR(Winsorization_knn_point7)),\n",
        "    'point7_Zscore': (z_score(Winsorization_knn_point7)),\n",
        "    'point7_isolationforest': (isolation_forest(Winsorization_knn_point7))\n",
        "}\n",
        "\n",
        "print(hold_out_point7_datasets.keys())"
      ],
      "metadata": {
        "id": "gQ-SzOR05X7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'point7_Zscore' in hold_out_point7_datasets:\n",
        "\n",
        "    display(hold_out_point7_datasets['point7_Zscore'].describe())\n",
        "else:\n",
        "    print(\"The key 'point7_Zscore' does not exist in the dictionary.\")"
      ],
      "metadata": {
        "id": "UyfGFwCgGu74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'point7_IQR' in hold_out_point7_datasets:\n",
        "\n",
        "    display(hold_out_point7_datasets['point7_IQR'].describe())\n",
        "else:\n",
        "    print(\"The key 'point7_IQR' does not exist in the dictionary.\")"
      ],
      "metadata": {
        "id": "6CGgcbidNOOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'point7_isolationforest' in hold_out_point7_datasets:\n",
        "\n",
        "    display(hold_out_point7_datasets['point7_isolationforest'].describe())\n",
        "else:\n",
        "    print(\"The key 'point7_isolationforest' does not exist in the dictionary.\")"
      ],
      "metadata": {
        "id": "Ijxn1WURNacC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = [ #it its from point7_Zscore_SMOTETomek_rfecv if i have use other data set if i use change this\n",
        "    'x0', 'y0', 'x1', 'y2', 'x3', 'y3', 'x4', 'x5', 'y5', 'x6', 'y6',\n",
        "    'x7', 'y7', 'x8', 'y8', 'x10', 'y10', 'x11', 'y11', 'x12', 'y12',\n",
        "    'x13', 'y13', 'y14', 'x17', 'T1', 'T2', 'T3', 'T4', 'T5', 'BAB',\n",
        "    'BAH', 'HB_index'\n",
        "]\n"
      ],
      "metadata": {
        "id": "p0ewh9EPV6FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hold_out_data_filtered_unscaled = hold_out_point7_datasets['point7_Zscore'][selected_features]"
      ],
      "metadata": {
        "id": "YYj1872yZ3wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hold_out_data_filtered_unscaled.describe().T"
      ],
      "metadata": {
        "id": "a7k9w1E5bEzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Used_in_model = 'point7_Zscore_SMOTETomek_rfecv' # change if i have use other version of my data ******\n",
        "x_train_scale, x_test_scale, y_train_scale, y_test_scale = XGBoost_outliers_variants_features_selected[Used_in_model]"
      ],
      "metadata": {
        "id": "gWnC_BctwLb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(x_test_scale)\n",
        "\n",
        "hold_out_data_filtered_unscaled = hold_out_point7_datasets['point7_Zscore'][selected_features]\n",
        "hold_out_data_filtered_unscaled = hold_out_data_filtered_unscaled.reindex(columns=x_test_scale.columns)\n",
        "\n",
        "try:\n",
        "    hold_out_data_filtered_scaled = pd.DataFrame(\n",
        "        scaler.transform(hold_out_data_filtered_unscaled),\n",
        "        columns=hold_out_data_filtered_unscaled.columns\n",
        "    )\n",
        "    print(\"Scaling successful.\")\n",
        "except ValueError as e:\n",
        "    print(\"Error during scaling:\", e)"
      ],
      "metadata": {
        "id": "e1TkYdTyhE2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hold_out_data_filtered_unscaled.head())\n",
        "print(hold_out_data_filtered_scaled.head())"
      ],
      "metadata": {
        "id": "jpKEKmnFl75g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean used by scaler: \", scaler.mean_)\n",
        "print(\"Scale used by scaler: \", scaler.scale_)"
      ],
      "metadata": {
        "id": "hKdsTVGAmRlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_best)\n",
        "\n",
        "hold_out_data_filtered_scaled = pd.DataFrame(\n",
        "    scaler.transform(hold_out_data_filtered_unscaled),\n",
        "    columns=hold_out_data_filtered_unscaled.columns\n",
        ")\n",
        "\n",
        "print(hold_out_data_filtered_scaled.describe().T)"
      ],
      "metadata": {
        "id": "NgLi26zrlbGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hold_out_data_filtered_unscaled.head())\n",
        "print(hold_out_data_filtered_scaled.head())"
      ],
      "metadata": {
        "id": "mf329kUToATG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a68jHa_1D7e"
      },
      "source": [
        "# Try to submitting it to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Used_in_model = 'point7_Zscore_SMOTETomek_rfecv'\n",
        "x_train_submit, x_test_submit, y_train_submit, y_test_submit = XGBoost_outliers_variants_features_selected[Used_in_model]"
      ],
      "metadata": {
        "id": "dV-DA3eVs5eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_submit.shape)"
      ],
      "metadata": {
        "id": "kaZ9gq7OxJZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train_submit)"
      ],
      "metadata": {
        "id": "x5jeVjjGtaHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_submit_scaled = pd.DataFrame(scaler.transform(x_train_submit), columns=x_train_submit.columns)\n",
        "x_train_submit_unscaled = x_train_submit.copy()"
      ],
      "metadata": {
        "id": "wdviEUwhyKWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_submit.shape)\n",
        "print(y_train_submit.shape)"
      ],
      "metadata": {
        "id": "fhA29RoqzLTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_submit = pd.Series(y_train_submit)\n",
        "y_train_submit.reset_index(drop=True, inplace=True)\n",
        "y_train_submit = y_train_submit.values"
      ],
      "metadata": {
        "id": "DCmpLD170lCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_XGB_After_proccess.fit(x_train_submit_unscaled, y_train_submit)\n",
        "best_GB_After_proccess.fit(x_train_submit_unscaled, y_train_submit)\n",
        "\n",
        "best_RF_After_proccess.fit(x_train_submit_scaled, y_train_submit)\n",
        "best_SVC_After_proccess.fit(x_train_submit_scaled, y_train_submit)"
      ],
      "metadata": {
        "id": "TDHBmXsOpLtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "best_ensemble_Voting_model submit"
      ],
      "metadata": {
        "id": "WCytjd_PhSMQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouWlEVDO6Nov"
      },
      "outputs": [],
      "source": [
        "training_columns = x_train_submit.columns\n",
        "hold_out_data_filtered_unscaled = hold_out_data_filtered_unscaled.reindex(columns=training_columns, fill_value=0)\n",
        "\n",
        "hold_out_data_filtered_scaled = pd.DataFrame(\n",
        "    scaler.transform(hold_out_data_filtered_unscaled),\n",
        "    columns=hold_out_data_filtered_unscaled.columns\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_pred = best_XGB_After_proccess.predict(hold_out_data_filtered_unscaled)\n",
        "gbc_pred = best_GB_After_proccess.predict(hold_out_data_filtered_unscaled)\n",
        "\n",
        "rf_pred = best_RF_After_proccess.predict(hold_out_data_filtered_unscaled)\n",
        "svm_pred = best_SVC_After_proccess.predict(hold_out_data_filtered_scaled)"
      ],
      "metadata": {
        "id": "Q3Y8dym62cpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hold_out_pred = best_ensemble_Voting_model.predict_proba(hold_out_data_filtered_scaled)[:, 1]"
      ],
      "metadata": {
        "id": "dgT3v6u32rAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9Fvk9UJb3I0"
      },
      "outputs": [],
      "source": [
        "RowID = np.array(hold_out_data_filtered_unscaled.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5L_pRdHFRQm"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame({'RowID': RowID, 'sex': hold_out_pred})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "id": "aPgzIEF0DmSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgnevXaXfsIR"
      },
      "outputs": [],
      "source": [
        "results.to_csv('results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys1FZF7-FS_B"
      },
      "outputs": [],
      "source": [
        "'''!kaggle competitions submit -c budm-24 -f results.csv -m 'test''''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "best_GB_After_proccess submit"
      ],
      "metadata": {
        "id": "mr8k4jlShBNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_GB_After_proccess.fit(x_train_submit_scaled, y_train_submit)\n",
        "\n",
        "gb_pred = best_GB_After_proccess.predict_proba(hold_out_data_filtered_scaled)[:, 1]\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'RowID': np.array(hold_out_data_filtered.index),\n",
        "    'Sex': gb_pred\n",
        "})\n",
        "\n",
        "results.to_csv('best_GB_After_proccess.csv', index=False)\n",
        "print(results.head)"
      ],
      "metadata": {
        "id": "k3RJKG2nat6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('best_GB_After_proccess.csv', index=False)"
      ],
      "metadata": {
        "id": "3SNMv33Wbl4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c budm-24 -f best_GB_After_proccess.csv -m 'best_GB_After_proccess_test'"
      ],
      "metadata": {
        "id": "5lDT3Yo7bqbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "best_XGB_After_proccess submit"
      ],
      "metadata": {
        "id": "4QzBrA01g78j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_XGB_After_proccess.fit(x_train_submit_scaled, y_train_submit)\n",
        "\n",
        "gb_pred = best_XGB_After_proccess.predict_proba(hold_out_data_filtered_scaled)[:, 1]\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'RowID': np.array(hold_out_data_filtered.index),\n",
        "    'Sex': gb_pred\n",
        "})\n",
        "\n",
        "results.to_csv('best_XGB_After_proccess.csv', index=False)\n",
        "print(results.head)"
      ],
      "metadata": {
        "id": "YfjNpfU0btQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('best_XGB_After_proccess.csv', index=False)"
      ],
      "metadata": {
        "id": "4LWG3wdDhYoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c budm-24 -f best_XGB_After_proccess.csv -m 'best_XGB_After_proccess_test"
      ],
      "metadata": {
        "id": "Spfi3DnQhiUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "best_RF_After_proccess submit"
      ],
      "metadata": {
        "id": "aLXFCazAhqek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_RF_After_proccess.fit(x_train_submit_scaled, y_train_submit)\n",
        "\n",
        "gb_pred = best_RF_After_proccess.predict_proba(hold_out_data_filtered_scaled)[:, 1]\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'RowID': np.array(hold_out_data_filtered.index),\n",
        "    'Sex': gb_pred\n",
        "})\n",
        "\n",
        "results.to_csv('best_RF_After_proccess.csv', index=False)\n",
        "print(results.head)"
      ],
      "metadata": {
        "id": "fOGDboW2h2XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('best_RF_After_proccess.csv', index=False)"
      ],
      "metadata": {
        "id": "NapPhmMRh7zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c budm-24 -f best_RF_After_proccess.csv -m 'best_RF_After_proccess_test'"
      ],
      "metadata": {
        "id": "CUayly1LiAwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "best_SVC_After_proccess submit"
      ],
      "metadata": {
        "id": "-c1IJl0SiDrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_SVC_After_proccess.fit(x_train_submit_scaled, y_train_submit)\n",
        "\n",
        "gb_pred = best_SVC_After_proccess.predict_proba(hold_out_data_filtered_scaled)[:, 1]\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'RowID': np.array(hold_out_data_filtered.index),\n",
        "    'Sex': gb_pred\n",
        "})\n",
        "\n",
        "results.to_csv('best_SVC_After_proccess.csv', index=False)\n",
        "print(results.head)"
      ],
      "metadata": {
        "id": "AjXxzFKJiXxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('best_SVC_After_proccess.csv', index=False)"
      ],
      "metadata": {
        "id": "WA_N9AidigFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c budm-24 -f best_SVC_After_proccess.csv -m 'best_SVC_After_proccess_test'"
      ],
      "metadata": {
        "id": "QZic7q4vigoL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}